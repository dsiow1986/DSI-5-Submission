title,company,employment_type,job_level,category,salary_lower,salary_upper,salary_time_interval,roles,requirements
"Strategic Cloud Data Engineer, Google Professional Services - Singapore",GOOGLE ASIA PACIFIC PTE. LTD.,Full Time,Executive,Engineering,6750.0,13500.0,Monthly,"Company overview: Google is not a conventional company, and we don’t intend to become one. True, we share attributes with the world’s most successful organizations – a focus on innovation and smart business practices comes to mind – but even as we continue to grow, we’re committed to retaining a small-company feel. At Google, we know that every employee has something important to say, and that every employee is integral to our success. We provide individually-tailored compensation packages that can be comprised of competitive salary, bonus, and equity components, along with the opportunity to earn further financial bonuses and rewards. Googlers thrive in small, focused teams and high-energy environments, believe in the ability of technology to change the world, and are as passionate about their lives as they are about their work. For more information, visit www.google.com/careers. The area: Google Cloud Google Cloud helps millions of employees and organizations empower their employees, serve their customers, and build what’s next for their business — all with technology built in the cloud. Our products are engineered for security, reliability and scalability, running the full stack from infrastructure to applications to devices and hardware. And our teams are dedicated to helping our customers — developers, small and large businesses, educational institutions and government agencies — see the benefits of our technology come to life. The role: Strategic Cloud Data Engineer, Google Professional Services - Singapore The Google Cloud Platform team helps customers transform and evolve their business through the use of Google’s global network, web-scale data centers and software infrastructure. As part of an entrepreneurial team in this rapidly growing business, you will help shape the future of businesses of all sizes use technology to connect with customers, employees and partners. Additional Role Description: As a Cloud Data Engineer, you'll guide customers on how to ingest, store, process, analyze and explore/visualize data on the Google Cloud Platform. You will work on data migrations and transformational projects and work with customers to design large-scale data processing systems, develop data pipelines optimized for scaling, and troubleshoot potential platform challenges. You will travel to customer sites to deploy solutions and deliver workshops to educate and empower customers. Additionally, you'll work closely with Product Management and Product Engineering teams to build and constantly drive excellence in our products. Responsibilities: - Be a trusted technical advisor to customers and solve complex cloud infrastructure and networking challenges. - Create and deliver best practices recommendations, tutorials, blog articles, sample code and technical presentations adapting to different levels of key business and technical stakeholders. - Travel regularly, up to 30% of the time in-region for meetings, technical reviews, and onsite delivery activities.","Company overview: Google is not a conventional company, and we don’t intend to become one. True, we share attributes with the world’s most successful organizations – a focus on innovation and smart business practices comes to mind – but even as we continue to grow, we’re committed to retaining a small-company feel. At Google, we know that every employee has something important to say, and that every employee is integral to our success. We provide individually-tailored compensation packages that can be comprised of competitive salary, bonus, and equity components, along with the opportunity to earn further financial bonuses and rewards. Googlers thrive in small, focused teams and high-energy environments, believe in the ability of technology to change the world, and are as passionate about their lives as they are about their work. For more information, visit www.google.com/careers. The area: Google Cloud Google Cloud helps millions of employees and organizations empower their employees, serve their customers, and build what’s next for their business — all with technology built in the cloud. Our products are engineered for security, reliability and scalability, running the full stack from infrastructure to applications to devices and hardware. And our teams are dedicated to helping our customers — developers, small and large businesses, educational institutions and government agencies — see the benefits of our technology come to life. The role: Strategic Cloud Data Engineer, Google Professional Services - Singapore The Google Cloud Platform team helps customers transform and evolve their business through the use of Google’s global network, web-scale data centers and software infrastructure. As part of an entrepreneurial team in this rapidly growing business, you will help shape the future of businesses of all sizes use technology to connect with customers, employees and partners. Additional Role Description: As a Cloud Data Engineer, you'll guide customers on how to ingest, store, process, analyze and explore/visualize data on the Google Cloud Platform. You will work on data migrations and transformational projects and work with customers to design large-scale data processing systems, develop data pipelines optimized for scaling, and troubleshoot potential platform challenges. You will travel to customer sites to deploy solutions and deliver workshops to educate and empower customers. Additionally, you'll work closely with Product Management and Product Engineering teams to build and constantly drive excellence in our products. Responsibilities: - Be a trusted technical advisor to customers and solve complex cloud infrastructure and networking challenges. - Create and deliver best practices recommendations, tutorials, blog articles, sample code and technical presentations adapting to different levels of key business and technical stakeholders. - Travel regularly, up to 30% of the time in-region for meetings, technical reviews, and onsite delivery activities."
Senior Data Engineer,THOUGHTWORKS PTE. LTD.,Full Time,Professional,"Consulting , Information Technology",5700.0,11200.0,Monthly,"Singapore, SingaporeThoughtWorks Singapore is looking for talented engineers passionate about building large scale data processing systems to help manage the ever-growing information needs of our clients.    You will be responsible for -   Creating complex data processing pipelines, as part of diverse, high energy teams Designing scalable implementations of the models  Hands-on programming based on TDD, usually in a pair programming environment Deploying data pipelines in production based on Continuous Delivery practices Advising clients on the usage of different distributed storage and computing technologies from the plethora of options available in the ecosystem ","Singapore, SingaporeThoughtWorks Singapore is looking for talented engineers passionate about building large scale data processing systems to help manage the ever-growing information needs of our clients.    You will be responsible for -   Creating complex data processing pipelines, as part of diverse, high energy teams Designing scalable implementations of the models  Hands-on programming based on TDD, usually in a pair programming environment Deploying data pipelines in production based on Continuous Delivery practices Advising clients on the usage of different distributed storage and computing technologies from the plethora of options available in the ecosystem "
Sr. Software engineer Data cap /  ICM,NIBAARA TECHNOLOGIES PTE. LTD.,Full Time,Professional,Information Technology,5000.0,7000.0,Monthly," Ensure the timely completion of the tasks assigned Ensure to follow the Technology and Process Standards, set for the project Produce high quality technical documentation Develop Low Level Design Undertake development tasks with minimal supervision, including programming and testing To monitor progress and provide timely updates to Lead On need basis should able to work as back up lead Technical Design documents Developed Programs with high quality and as per the planned schedule Developed Programs with high quality, with process standards and as per the planned schedule Unit Test Cases Interacts with Technical Lead, BAs and testers for the project during the project lifecycle Interacts with the Client managers, for status review meetings "," Ensure the timely completion of the tasks assigned Ensure to follow the Technology and Process Standards, set for the project Produce high quality technical documentation Develop Low Level Design Undertake development tasks with minimal supervision, including programming and testing To monitor progress and provide timely updates to Lead On need basis should able to work as back up lead Technical Design documents Developed Programs with high quality and as per the planned schedule Developed Programs with high quality, with process standards and as per the planned schedule Unit Test Cases Interacts with Technical Lead, BAs and testers for the project during the project lifecycle Interacts with the Client managers, for status review meetings "
Data cap /  ICM Engineer,NIBAARA TECHNOLOGIES PTE. LTD.,Full Time,Professional,Information Technology,4500.0,6000.0,Monthly," Hands on experience in designing and developing applications using Datacap 9.1. Profound insight of dojo, navigator, VB script and Java script. Working experience with Relational Databases, PL/SQL. Experience in Oracle preferred. Experience in Datacap solution deployment. Object oriented analysis and design using common design patterns. Good to have experience in designing and developing applications using Datacap 8.1, ASP.net and VB script. Good to have knowledge of OCR (optical character recognition) Technical Design documents Developed Programs with high quality and as per the planned schedule Developed Programs with high quality, with process standards and as per the planned schedule Unit Test Cases   "," Hands on experience in designing and developing applications using Datacap 9.1. Profound insight of dojo, navigator, VB script and Java script. Working experience with Relational Databases, PL/SQL. Experience in Oracle preferred. Experience in Datacap solution deployment. Object oriented analysis and design using common design patterns. Good to have experience in designing and developing applications using Datacap 8.1, ASP.net and VB script. Good to have knowledge of OCR (optical character recognition) Technical Design documents Developed Programs with high quality and as per the planned schedule Developed Programs with high quality, with process standards and as per the planned schedule Unit Test Cases   "
Data Engineer,Company Undisclosed,Permanent,"Executive, Senior Executive",Banking and Finance,6500.0,13000.0,Monthly,"Position Purpose The APAC GM & ALMT IT TP Engineering group runs Trade Processing and Regulatory Projects for Global Market and ALMT business and operations.   Responsibilities  Partner with senior business sponsors, platform supervision, compliance, and business management to enhance and augment the holistic supervisory and surveillance platform Investigate and analyze data on the surveillance platform as key input to enhance existing data strategy on multi-vector analysis using various natural language processing and machine learning techniques Primary focus is to investigate and analyze available structured and un-structured information, apply data mining techniques, perform statistical analysis, and provide methods (algorithmic and technical) to create actionable alerts and reduce false-positives Develop and enhance software solutions to meet business requirements Gather and document technical requirements and specifications Work with the project PM and BA’s on project planning and deliverable Work on multiple tasks and respect aggressive schedule Work in a fast paced environment. ","Position Purpose The APAC GM & ALMT IT TP Engineering group runs Trade Processing and Regulatory Projects for Global Market and ALMT business and operations.   Responsibilities  Partner with senior business sponsors, platform supervision, compliance, and business management to enhance and augment the holistic supervisory and surveillance platform Investigate and analyze data on the surveillance platform as key input to enhance existing data strategy on multi-vector analysis using various natural language processing and machine learning techniques Primary focus is to investigate and analyze available structured and un-structured information, apply data mining techniques, perform statistical analysis, and provide methods (algorithmic and technical) to create actionable alerts and reduce false-positives Develop and enhance software solutions to meet business requirements Gather and document technical requirements and specifications Work with the project PM and BA’s on project planning and deliverable Work on multiple tasks and respect aggressive schedule Work in a fast paced environment. "
"VP / AVP, Senior Data Engineer, Group Consumer Banking and Big Data Analytics Technology (180003L2)",DBS BANK LTD.,"Permanent, Full Time","Middle Management, Manager",Banking and Finance,7000.0,14000.0,Monthly," Design and implement key components for highly scalable, distributed data collection and analysis system built for handling petabytes of data in the cloud.  Work with architects from other divisions contributing to this analytics system and mentor team members on best practices in backend infrastructure and distributed computing topics.  Analyze source data and data flows, working with structured and unstructured data. Manipulate high-volume, high-dimensionality data from varying sources to highlight patterns, anomalies, relationships and trends Analyze and visualize diverse sources of data, interpret results in the business context and report results clearly and concisely. Apply data mining, NLP, and machine learning (both supervised and unsupervised) to improve relevance and personalization algorithms. Work side-by-side with product managers, software engineers, and designers in designing experiments and minimum viable products. Build and optimize classifiers using machine learning techniques and enhance data collection procedures that is relevant for building analytic systems. Discover data sources, get access to them, import them, clean them up, and make them “model-ready”. You need to be willing and able to do your own ETL. Create and refine features from the underlying data. You’ll enjoy developing just enough subject matter expertise to have an intuition about what features might make your model perform better, and then you’ll lather, rinse and repeat. Run regular A/B tests, gather data, perform statistical analysis, draw conclusions on the impact of your optimizations and communicate results to peers and leaders. "," Design and implement key components for highly scalable, distributed data collection and analysis system built for handling petabytes of data in the cloud.  Work with architects from other divisions contributing to this analytics system and mentor team members on best practices in backend infrastructure and distributed computing topics.  Analyze source data and data flows, working with structured and unstructured data. Manipulate high-volume, high-dimensionality data from varying sources to highlight patterns, anomalies, relationships and trends Analyze and visualize diverse sources of data, interpret results in the business context and report results clearly and concisely. Apply data mining, NLP, and machine learning (both supervised and unsupervised) to improve relevance and personalization algorithms. Work side-by-side with product managers, software engineers, and designers in designing experiments and minimum viable products. Build and optimize classifiers using machine learning techniques and enhance data collection procedures that is relevant for building analytic systems. Discover data sources, get access to them, import them, clean them up, and make them “model-ready”. You need to be willing and able to do your own ETL. Create and refine features from the underlying data. You’ll enjoy developing just enough subject matter expertise to have an intuition about what features might make your model perform better, and then you’ll lather, rinse and repeat. Run regular A/B tests, gather data, perform statistical analysis, draw conclusions on the impact of your optimizations and communicate results to peers and leaders. "
Data Engineer,HYDROINFORMATICS INSTITUTE PTE. LTD.,Full Time,Executive,Professional Services,,,,"Roles and Responsibilities:  Implement data wrangling, preprocessing and preparation scripts for available raw environmental data Implement statistical and machine learning algorithms for data analysis and knowledge derivation Implement data visualization solutions for post processing and communication to internal and external teams Create and maintain databases for different kinds of geo-spatiotemporal data Monitor and manage data pipelines and test production codes ","Roles and Responsibilities:  Implement data wrangling, preprocessing and preparation scripts for available raw environmental data Implement statistical and machine learning algorithms for data analysis and knowledge derivation Implement data visualization solutions for post processing and communication to internal and external teams Create and maintain databases for different kinds of geo-spatiotemporal data Monitor and manage data pipelines and test production codes "
Cyber Security Big Data Engineer,Company Undisclosed,Full Time,Professional,Information Technology,7000.0,12000.0,Monthly,"Working in Cybersecurity takes pure passion for technology, speed, a constant desire to learn, and above all, vigilance in keeping every last asset safe and sound. You’ll be on the front lines of innovation, working with a highly-motivated team laser-focused on analyzing, designing, developing and delivering solutions built to stop adversaries and strengthen our operations. Your research and work will ensure stability, capacity and resiliency of our products in emerging industry trends. Working in tandem with your internal team, as well as technologists and innovators across our global network, your ability to identify threats, provide intelligent analysis and positive actions will stop adversaries and strengthen our products.   Responsibilities  Focus on the development of tools and technologies that are at the core of the company’s capabilities to manage, monitor and hunt for cyber security incidents Architecture and development of large scale solution (big data) to be used in a very large production environment System, network and application troubleshooting Provide engineering support for cyber security products developed ","Working in Cybersecurity takes pure passion for technology, speed, a constant desire to learn, and above all, vigilance in keeping every last asset safe and sound. You’ll be on the front lines of innovation, working with a highly-motivated team laser-focused on analyzing, designing, developing and delivering solutions built to stop adversaries and strengthen our operations. Your research and work will ensure stability, capacity and resiliency of our products in emerging industry trends. Working in tandem with your internal team, as well as technologists and innovators across our global network, your ability to identify threats, provide intelligent analysis and positive actions will stop adversaries and strengthen our products.   Responsibilities  Focus on the development of tools and technologies that are at the core of the company’s capabilities to manage, monitor and hunt for cyber security incidents Architecture and development of large scale solution (big data) to be used in a very large production environment System, network and application troubleshooting Provide engineering support for cyber security products developed "
"VP / AVP, Machine Learning Engineer, Group Consumer Banking and Big Data Analytics Tech (180003YE)",DBS BANK LTD.,"Permanent, Full Time","Middle Management, Manager",Banking and Finance,7000.0,14000.0,Monthly,"Job Purpose    Build and improve machine learning and analytics platform. Work with data scientists to create, optimize and productionize of machine learning models for various business units within the organization. Keep innovating and optimizing data and machine learning workflow to enable data-driven business activities at large scale.    Responsibilities   Build and improve machine learning and analytics platform.       Apply cutting edge technologies and tool chain in big data and machine learning to build machine learning and analytics platform. Keep innovating and optimizing the machine learning workflow, from data exploration, model experimentation/prototyping to production. Provide engineering solution and framework to support machine learning and data-driven business activities at large scale. Perform R&D on new technologies and solutions to improve accessibility, scalability, efficiency and us abilities of machine learning and analytics platform.     Work with data scientists to build end-to-end machine learning and analytics solution to solve business challenges.       Turn advanced machine learning models created by data scientists into end-to-end production grade system. Build analytics platform components to support data collection, exploratory, and integration from various sources being data API, RDBMS, or big data platform. Optimize efficiency of machine learning algorithm by applying state-of-the-art technologies, i.e. distributed computing, concurrent programming, or GPU parallel computing.      Establish, apply and maintain best practices and principles of machine learning engineering.       Study and evaluate the state of the art technologies, tools, and frameworks of machine learning engineering. Contribute in creation of blueprint and reference architecture for various machine learning use cases. Support the organization in transformation towards a data driven business culture.     Work Relationships  Internal        Work closely with data scientists, business team, and project managers to provide machine learning and data-driven business solution.  Collaborate with other technology teams to build platform and framework to enable machine learning and data analytics activities at large scale     External        Maintain engineering principles and best practices of machine learning framework and technologies.   ","Job Purpose    Build and improve machine learning and analytics platform. Work with data scientists to create, optimize and productionize of machine learning models for various business units within the organization. Keep innovating and optimizing data and machine learning workflow to enable data-driven business activities at large scale.    Responsibilities   Build and improve machine learning and analytics platform.       Apply cutting edge technologies and tool chain in big data and machine learning to build machine learning and analytics platform. Keep innovating and optimizing the machine learning workflow, from data exploration, model experimentation/prototyping to production. Provide engineering solution and framework to support machine learning and data-driven business activities at large scale. Perform R&D on new technologies and solutions to improve accessibility, scalability, efficiency and us abilities of machine learning and analytics platform.     Work with data scientists to build end-to-end machine learning and analytics solution to solve business challenges.       Turn advanced machine learning models created by data scientists into end-to-end production grade system. Build analytics platform components to support data collection, exploratory, and integration from various sources being data API, RDBMS, or big data platform. Optimize efficiency of machine learning algorithm by applying state-of-the-art technologies, i.e. distributed computing, concurrent programming, or GPU parallel computing.      Establish, apply and maintain best practices and principles of machine learning engineering.       Study and evaluate the state of the art technologies, tools, and frameworks of machine learning engineering. Contribute in creation of blueprint and reference architecture for various machine learning use cases. Support the organization in transformation towards a data driven business culture.     Work Relationships  Internal        Work closely with data scientists, business team, and project managers to provide machine learning and data-driven business solution.  Collaborate with other technology teams to build platform and framework to enable machine learning and data analytics activities at large scale     External        Maintain engineering principles and best practices of machine learning framework and technologies.   "
Lead Data Engineer (Established FinTech Co / East),PEOPLE PROFILERS PTE. LTD.,Permanent,Senior Executive,Information Technology,7500.0,10000.0,Monthly," Join a rapidly expanding and reputable company in FinTech industry  Attractive remuneration package Great benefits (work-life balance, fun work environment, health insurance, dental plan) Junior & senior positions available    Position Overview:  Support our team in developing cutting-edge solutions for the FinTech industry, keeping security standards in mind.    Responsibilities:  Building robust batch and streaming data pipeline for production-grade data products/platforms (tools: Hadoop, Spark) Creating web services or APIs to connect analytical stacks to application layers Building and maintaining both cloud and on-premise data infrastructure Data cleaning, & pre-processing (e.g. with images/text) Analyse requirements and deliver suitable solutions Write code according to best practices, and which meets security standards Keep up to date with new technologies "," Join a rapidly expanding and reputable company in FinTech industry  Attractive remuneration package Great benefits (work-life balance, fun work environment, health insurance, dental plan) Junior & senior positions available    Position Overview:  Support our team in developing cutting-edge solutions for the FinTech industry, keeping security standards in mind.    Responsibilities:  Building robust batch and streaming data pipeline for production-grade data products/platforms (tools: Hadoop, Spark) Creating web services or APIs to connect analytical stacks to application layers Building and maintaining both cloud and on-premise data infrastructure Data cleaning, & pre-processing (e.g. with images/text) Analyse requirements and deliver suitable solutions Write code according to best practices, and which meets security standards Keep up to date with new technologies "
Data Engineer,SANDBOX CONSULTING PTE. LTD.,"Permanent, Contract, Full Time",Professional,Information Technology,5500.0,6000.0,Monthly," Design, build, launch and maintain efficient and reliable large-scale batch and real-time data pipelines with data processing frameworks  Integrate and collate data silos in a manner which is both scalable and compliant  Collaborate with Project Manager, Frontend Developers, UX Designers and Data Analyst to build scalable data-driven products  Responsible for developing backend APIs & working on databases to support the applications  Working in an Agile Environment that practices Continuous Integration and Delivery  Working closely with fellow developers through pair programming and code review process"," Design, build, launch and maintain efficient and reliable large-scale batch and real-time data pipelines with data processing frameworks  Integrate and collate data silos in a manner which is both scalable and compliant  Collaborate with Project Manager, Frontend Developers, UX Designers and Data Analyst to build scalable data-driven products  Responsible for developing backend APIs & working on databases to support the applications  Working in an Agile Environment that practices Continuous Integration and Delivery  Working closely with fellow developers through pair programming and code review process"
Data Engineer,Company Undisclosed,"Contract, Full Time",Executive,Information Technology,5000.0,7000.0,Monthly," Design, build, launch and maintain efficient and reliable large-scale batch and real-time data pipelines with data processing frameworks Integrate and collate data silos in a manner which is both scalable and compliant Collaborate with Project Manager, Frontend Developers, UX Designers and Data Analyst to build scalable data-driven products Responsible for developing backend APIs & working on databases to support the applications Working in an Agile Environment that practices Continuous Integration and Delivery Working closely with fellow developers through pair programming and code review process "," Design, build, launch and maintain efficient and reliable large-scale batch and real-time data pipelines with data processing frameworks Integrate and collate data silos in a manner which is both scalable and compliant Collaborate with Project Manager, Frontend Developers, UX Designers and Data Analyst to build scalable data-driven products Responsible for developing backend APIs & working on databases to support the applications Working in an Agile Environment that practices Continuous Integration and Delivery Working closely with fellow developers through pair programming and code review process "
APAC Data Engineer,UPS ASIA GROUP PTE. LTD.,Permanent,Senior Executive,Logistics / Supply Chain,5256.0,7008.0,Monthly,"Summary The Data Engineer will be responsible for expanding and optimizing the data and data pipeline architecture, data flow and collection for the Data Science team and creating API’s to integrate the models with production systems. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support the data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. Responsibilities:  Create and maintain optimal data pipeline architecture. Assemble large, complex data sets from multiple data sources that meet functional / nonfunctional business requirements. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and no SQL technologies. Develop data features that will serve as inputs to AI/Machine Learning/OR techniques. Build analytics tools that utilize the data pipeline to provide actionable insights into key business performance metrics. Develop data design based on exploratory data analysis to meet stated business need. Develop procedures to monitor model and production system performance/integrity. Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Review and create repeatable solutions through written project documentation, process flowcharts, logs, and commented clean code to produce datasets that can be used in analytics and/or predictive modeling. Act as subject matter expert with investigating and evaluating emerging technologies. Articulate potential competitive market benefits of new technologies to senior management. Maintain broad understanding of implementation, integration, and interconnectivity issues with emerging technologies. ","Summary The Data Engineer will be responsible for expanding and optimizing the data and data pipeline architecture, data flow and collection for the Data Science team and creating API’s to integrate the models with production systems. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support the data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. Responsibilities:  Create and maintain optimal data pipeline architecture. Assemble large, complex data sets from multiple data sources that meet functional / nonfunctional business requirements. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and no SQL technologies. Develop data features that will serve as inputs to AI/Machine Learning/OR techniques. Build analytics tools that utilize the data pipeline to provide actionable insights into key business performance metrics. Develop data design based on exploratory data analysis to meet stated business need. Develop procedures to monitor model and production system performance/integrity. Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc. Review and create repeatable solutions through written project documentation, process flowcharts, logs, and commented clean code to produce datasets that can be used in analytics and/or predictive modeling. Act as subject matter expert with investigating and evaluating emerging technologies. Articulate potential competitive market benefits of new technologies to senior management. Maintain broad understanding of implementation, integration, and interconnectivity issues with emerging technologies. "
"AVP  /  Senior Associate, Data Engineer, IBG Digital, Institutional Banking Group (1800044U)",DBS BANK LTD.,"Permanent, Full Time","Manager, Senior Executive",Banking and Finance,5500.0,11000.0,Monthly,"Job Purpose  The Data Engineer will provide big data engineering support to the Institutional Banking Group (IBG) Business Analytics Team in various data science projects. This role’s primary job responsibility is defining the framework and process for preparing data for analytical uses. The right candidate will be one excited by the prospect of designing data engineering solutions from ground up and will support data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. Responsibilities  Create and maintain optimal data pipeline architecture; Assemble large, complex data sets that meet functional / non-functional business requirements; Identify, design, and implement internal process improvements: automating manual processes, Perform ETL/ELT, Data Modelling, Data Profiling, Data Cleansing, Feature Engineering tasks as part of Data Analytics Life Cycle (DALC); Work with stakeholders (data analysts, data scientists, technology support team) to assist with data-related technical issues and support data infrastructure needs; Build processes supporting data transformation, data structures, dependency and workload management ","Job Purpose  The Data Engineer will provide big data engineering support to the Institutional Banking Group (IBG) Business Analytics Team in various data science projects. This role’s primary job responsibility is defining the framework and process for preparing data for analytical uses. The right candidate will be one excited by the prospect of designing data engineering solutions from ground up and will support data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. Responsibilities  Create and maintain optimal data pipeline architecture; Assemble large, complex data sets that meet functional / non-functional business requirements; Identify, design, and implement internal process improvements: automating manual processes, Perform ETL/ELT, Data Modelling, Data Profiling, Data Cleansing, Feature Engineering tasks as part of Data Analytics Life Cycle (DALC); Work with stakeholders (data analysts, data scientists, technology support team) to assist with data-related technical issues and support data infrastructure needs; Build processes supporting data transformation, data structures, dependency and workload management "
"AVP  /  Senior Associate, Data Engineer, Analytic Center of Excellence, Transformation Grp (180003G6)",DBS BANK LTD.,"Permanent, Full Time","Manager, Senior Executive",Banking and Finance,5500.0,11000.0,Monthly,"Job Purpose The Data Engineer will provide big data engineering support to the Analytic Center of Excellence.  This role’s primary job responsibility is defining the framework and process for preparing data for analytical uses. The right candidate will be excited by the prospect of designing data engineering solutions from ground up and will support data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects.   Responsibilities  Create and maintain optimal data pipeline architecture; Assemble large, complex data sets that meet functional / non-functional business requirements; Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, designing infrastructure for greater scalability, etc. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using ‘big data’ technologies; Work with stakeholders (data analysts, data scientists, technology support team) to assist with data-related technical issues and support data infrastructure needs; Build processes supporting data transformation, data structures, dependency and workload management ","Job Purpose The Data Engineer will provide big data engineering support to the Analytic Center of Excellence.  This role’s primary job responsibility is defining the framework and process for preparing data for analytical uses. The right candidate will be excited by the prospect of designing data engineering solutions from ground up and will support data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects.   Responsibilities  Create and maintain optimal data pipeline architecture; Assemble large, complex data sets that meet functional / non-functional business requirements; Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, designing infrastructure for greater scalability, etc. Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using ‘big data’ technologies; Work with stakeholders (data analysts, data scientists, technology support team) to assist with data-related technical issues and support data infrastructure needs; Build processes supporting data transformation, data structures, dependency and workload management "
Data Engineer,WORKATO PTE. LTD.,"Permanent, Full Time",Executive,"Engineering, Information Technology",2000.0,10000.0,Monthly,"Role We are seeking a talented, self-directed Data Engineer to design, develop, implement, test, document, and operate large-scale, high-volume, high-performance data structures for our business stakeholders. Implement data structures using best practices in data modeling and ETL/ELT processes. Gather business and functional requirements and translate these requirements into robust, scalable, operable solutions that work well within the overall data architecture. Analyze source data systems and drive best practices in source teams. Participate in the full development life cycle, end-to-end, from design, implementation and testing, to documentation, delivery, support, and maintenance. Produce comprehensive, usable dataset documentation and metadata. Evaluate and make decisions around dataset implementations designed and proposed by peer data engineers. Evaluate and make decisions around the use of new or existing software products and tools. Mentor junior data engineers.  The ideal candidate relishes working with data, enjoys the challenge of highly complex technical contexts, and, above all else, is passionate about data and analytics. He/she is an expert with data modeling, ETL design and business intelligence tools and passionately partners with the business to identify strategic opportunities where improvements in data infrastructure creates outsized business impact. He/she is a self-starter, comfortable with ambiguity, able to think big (while paying careful attention to detail) and enjoys working in a fast-paced team. The ideal candidate needs to possess exceptional technical expertise in large scale data warehouse and BI systems with hands-on knowledge on SQL, Distributed/MPP data storage, and AWS services (S3, Redshift, EMR, RDS).   Responsibilities   Design, implement, and support a platform providing ad hoc access to large datasets   Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL   Implement data structures using best practices in data modeling, ETL/ELT processes, and SQL, and Redshift   Build robust and scalable data integration (ETL) pipelines using SQL, Python and Spark   Build and deliver high quality datasets to support business analysis and customer reporting needs   Interface with business customers, gathering requirements and delivering complete data structures  ","Role We are seeking a talented, self-directed Data Engineer to design, develop, implement, test, document, and operate large-scale, high-volume, high-performance data structures for our business stakeholders. Implement data structures using best practices in data modeling and ETL/ELT processes. Gather business and functional requirements and translate these requirements into robust, scalable, operable solutions that work well within the overall data architecture. Analyze source data systems and drive best practices in source teams. Participate in the full development life cycle, end-to-end, from design, implementation and testing, to documentation, delivery, support, and maintenance. Produce comprehensive, usable dataset documentation and metadata. Evaluate and make decisions around dataset implementations designed and proposed by peer data engineers. Evaluate and make decisions around the use of new or existing software products and tools. Mentor junior data engineers.  The ideal candidate relishes working with data, enjoys the challenge of highly complex technical contexts, and, above all else, is passionate about data and analytics. He/she is an expert with data modeling, ETL design and business intelligence tools and passionately partners with the business to identify strategic opportunities where improvements in data infrastructure creates outsized business impact. He/she is a self-starter, comfortable with ambiguity, able to think big (while paying careful attention to detail) and enjoys working in a fast-paced team. The ideal candidate needs to possess exceptional technical expertise in large scale data warehouse and BI systems with hands-on knowledge on SQL, Distributed/MPP data storage, and AWS services (S3, Redshift, EMR, RDS).   Responsibilities   Design, implement, and support a platform providing ad hoc access to large datasets   Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL   Implement data structures using best practices in data modeling, ETL/ELT processes, and SQL, and Redshift   Build robust and scalable data integration (ETL) pipelines using SQL, Python and Spark   Build and deliver high quality datasets to support business analysis and customer reporting needs   Interface with business customers, gathering requirements and delivering complete data structures  "
Data Engineer,OBSERVATIONAL AND PRAGMATIC RESEARCH INSTITUTE PTE. LTD.,"Permanent, Full Time",Junior Executive,"Engineering, Information Technology",3000.0,6000.0,Monthly,"The Company OPRI is an academic research institution striving to improve the lives of patients through global research. OPRI has been leading the paradigm shift in real world evidence for the past 12 years, by delivering pragmatic clinical trials, disease registries and database research. The Role We are looking for a Data Engineer to work alongside our research, statistical and database teams in the UK, Singapore and Australia (Brisbane). In this position you will gain invaluable experience within an internationally recognised research organisation involved in analysis and dissemination of data from large-scale observational studies and pragmatic randomised controlled trials. The successful candidate will have high attention to detail, strong time management skills, and most importantly experience in the management and engineering of relational databases. Your responsibilities  Design, construct, install, test and maintain data collection and management systems:     Integrate data management technologies and software engineering tools for custom data collection applications Programming knowledge: Employ a variety of languages and tools (e.g. scripting languages) to combine systems together Ensure seamless integration of data across multiple databases       SQL, queries   Building APIs for data consumption Integrating external or new datasets into existing data pipelines Continuously monitoring and testing the system to ensure optimized performance   Build and maintain data collection platforms for specific organisational projects     Set up automated integration processes for Patient Reported Outcomes into various data collection platforms   EMR/EDC integration with Registry Database     Data collected via Registry EDCs to be uploaded into EMRs Data collected via site specific EMRs/EDCs to be uploaded into Registry EDCs    The role is for a permanent full-time position. Salary is dependent on qualifications and experience. Immediate start is available.","The Company OPRI is an academic research institution striving to improve the lives of patients through global research. OPRI has been leading the paradigm shift in real world evidence for the past 12 years, by delivering pragmatic clinical trials, disease registries and database research. The Role We are looking for a Data Engineer to work alongside our research, statistical and database teams in the UK, Singapore and Australia (Brisbane). In this position you will gain invaluable experience within an internationally recognised research organisation involved in analysis and dissemination of data from large-scale observational studies and pragmatic randomised controlled trials. The successful candidate will have high attention to detail, strong time management skills, and most importantly experience in the management and engineering of relational databases. Your responsibilities  Design, construct, install, test and maintain data collection and management systems:     Integrate data management technologies and software engineering tools for custom data collection applications Programming knowledge: Employ a variety of languages and tools (e.g. scripting languages) to combine systems together Ensure seamless integration of data across multiple databases       SQL, queries   Building APIs for data consumption Integrating external or new datasets into existing data pipelines Continuously monitoring and testing the system to ensure optimized performance   Build and maintain data collection platforms for specific organisational projects     Set up automated integration processes for Patient Reported Outcomes into various data collection platforms   EMR/EDC integration with Registry Database     Data collected via Registry EDCs to be uploaded into EMRs Data collected via site specific EMRs/EDCs to be uploaded into Registry EDCs    The role is for a permanent full-time position. Salary is dependent on qualifications and experience. Immediate start is available."
Big Data Engineer,SCHELLDEN GLOBAL PTE. LTD.,Full Time,Middle Management,Information Technology,4000.0,8000.0,Monthly,"Massive data: You will source / examine, analyze, engineer data pipelines for gigabytes/terabytes of structured and unstructured data with our platform to create value for customers.  Pushing the limits: This role will be on the cutting edge of our Data / Machine Learning platform. As we push to solve more of our customer challenges, you will be prototyping new features, tools and ideas. Innovate at a very fast pace to maintain our competitive edge.  Linux hacking: You will be masterfully using the command line, including tools like vi/emacs and understanding beyond basics of grep, bash, awk, sed, etc to aggressively dive into data, systems, and compute platforms to get the results you are seeking.  Production deployment: You will be responsible for integration and deployment of the machine learning pipelines into production where your ideas can come to life.  Coordinate and work with cross functional teams, sometimes located at different geo locations.","Massive data: You will source / examine, analyze, engineer data pipelines for gigabytes/terabytes of structured and unstructured data with our platform to create value for customers.  Pushing the limits: This role will be on the cutting edge of our Data / Machine Learning platform. As we push to solve more of our customer challenges, you will be prototyping new features, tools and ideas. Innovate at a very fast pace to maintain our competitive edge.  Linux hacking: You will be masterfully using the command line, including tools like vi/emacs and understanding beyond basics of grep, bash, awk, sed, etc to aggressively dive into data, systems, and compute platforms to get the results you are seeking.  Production deployment: You will be responsible for integration and deployment of the machine learning pipelines into production where your ideas can come to life.  Coordinate and work with cross functional teams, sometimes located at different geo locations."
Data Engineer,PERX TECHNOLOGIES PTE. LTD.,Permanent,Executive,Information Technology,5000.0,8000.0,Monthly,"What You’ll Do:   As a Data Engineer on the Analytics team, you will be the “source of truth” for Perx’s most fundamental data - such as end-customer engagement and client usage data - along with core metrics such as daily (DAU) and monthly active users (MAU). Alongside designing & implementing the plumbing & infrastructure that will power the Analytics frameworks, you will also help lead the company’s decision to use bleeding-edge data technologies and features, working directly with our infrastructure team to integrate them into the services you design at scale. In doing so, you will help empower the Engineering department, tens of co-workers, thousands of marketing analysts and millions of end customers to dream of new insights and new possibilities.   Who You Are: You are a go-getter & dreamer, wanting to join a community of extremely talented, forward-thinking & diverse engineers in the industry & region. You gain happiness in building & scaling resilient, robust, well performing, and end-to-end tested distributed systems that can power the most business-critical applications. You want to learn, work with, and leverage on cutting-edge open-source technologies. The ideal candidate has experience with and/or history of contributions to Python, Hadoop, Spark, Redshift, Cassandra, PostGREs, Ruby (on Rails) or similar technologies. You have experience in distributed systems, database internals, or performance analysis.","What You’ll Do:   As a Data Engineer on the Analytics team, you will be the “source of truth” for Perx’s most fundamental data - such as end-customer engagement and client usage data - along with core metrics such as daily (DAU) and monthly active users (MAU). Alongside designing & implementing the plumbing & infrastructure that will power the Analytics frameworks, you will also help lead the company’s decision to use bleeding-edge data technologies and features, working directly with our infrastructure team to integrate them into the services you design at scale. In doing so, you will help empower the Engineering department, tens of co-workers, thousands of marketing analysts and millions of end customers to dream of new insights and new possibilities.   Who You Are: You are a go-getter & dreamer, wanting to join a community of extremely talented, forward-thinking & diverse engineers in the industry & region. You gain happiness in building & scaling resilient, robust, well performing, and end-to-end tested distributed systems that can power the most business-critical applications. You want to learn, work with, and leverage on cutting-edge open-source technologies. The ideal candidate has experience with and/or history of contributions to Python, Hadoop, Spark, Redshift, Cassandra, PostGREs, Ruby (on Rails) or similar technologies. You have experience in distributed systems, database internals, or performance analysis."
Big Data Engineer,6ESTATES PTE. LTD.,Full Time,Professional,Information Technology,4000.0,8000.0,Monthly,"We are looking for the right individual who has the passion and desire to crack the code for this very hot field in AI and Big Data. 6Estates engineers build tools and solutions that ensure the delivery of high quality software for our stakeholders. For someone who wants to learn and grow, this role provides you the unique opportunity to work along with all the experts of different fields. As a Big Data Engineer, you will work with a team of talents to design, architect, and develop on the big data analytics platform. You will also touch on researching, introducing modern technologies and integrating your amazing innovations and ideas into our production systems.","We are looking for the right individual who has the passion and desire to crack the code for this very hot field in AI and Big Data. 6Estates engineers build tools and solutions that ensure the delivery of high quality software for our stakeholders. For someone who wants to learn and grow, this role provides you the unique opportunity to work along with all the experts of different fields. As a Big Data Engineer, you will work with a team of talents to design, architect, and develop on the big data analytics platform. You will also touch on researching, introducing modern technologies and integrating your amazing innovations and ideas into our production systems."
"AVP / Senior Associate, Lead Development Engineer, Grp Consumer Banking & Big Data Analytics (180002YV",DBS BANK LTD.,"Permanent, Full Time","Manager, Senior Executive",Banking and Finance,5500.0,11000.0,Monthly," Manage the Avaloq Configuration / Release Management tasks and work on the BAU tasks in non-production environments Env Planning (DB, application), Avaloq ICE Streaming, Release calendars preparation and communicate with all stakeholders Definition of Connect Direct (NDM), IBM MQ, Avaloq Tools Upgrade, TWS Definition and Batch support, and FIX Platform interface setup Projects and Enhancement requests. Plan and manage the end-to-end deployment and delivery of IT infrastructure for Bank’s project from deployment planning, setup & testing, pre-production readiness to production cutover. Collaborate with Architecture and Engineering team, Application teams, Infrastructure teams, and service providers in delivering quality IT solutions and services to meet business objectives. Ensure the project meet schedule and within the allocated budget and resources. Adhere to the bank's Project Management, Deployment and Change management process.  Prepare and submit the necessary change requests and requisition forms for the deployment of infrastructure for the project, such as facility request, IP/DNS/Hostname request, SAN requisition, etc. Conduct proper transition from Project to Operations before project closure. Prepare all project documentation such as SOM and status reports, assure report accuracy and timeliness. e.g. Weekly Project Status report, etc. Work collaboratively with technology team and vendors, provide single point of contact and drive resolution of issues that arise in projects. Maintain business partnership with Line of Business (LOBs) and constantly collect and manage user’s demands and applications’ infra requirements. Coordinate with DBA, Solaris, Linux, Windows, Network, ID Mgmt and other Infra admin teams on the system issue related tasks Co-ordinate with various Infra teams to apply the OS / DB / MQ patches  Apply the innovative thinking to automate the manual tasks, provide infrastructure services effortlessly, improve performance and resilience of the systems. "," Manage the Avaloq Configuration / Release Management tasks and work on the BAU tasks in non-production environments Env Planning (DB, application), Avaloq ICE Streaming, Release calendars preparation and communicate with all stakeholders Definition of Connect Direct (NDM), IBM MQ, Avaloq Tools Upgrade, TWS Definition and Batch support, and FIX Platform interface setup Projects and Enhancement requests. Plan and manage the end-to-end deployment and delivery of IT infrastructure for Bank’s project from deployment planning, setup & testing, pre-production readiness to production cutover. Collaborate with Architecture and Engineering team, Application teams, Infrastructure teams, and service providers in delivering quality IT solutions and services to meet business objectives. Ensure the project meet schedule and within the allocated budget and resources. Adhere to the bank's Project Management, Deployment and Change management process.  Prepare and submit the necessary change requests and requisition forms for the deployment of infrastructure for the project, such as facility request, IP/DNS/Hostname request, SAN requisition, etc. Conduct proper transition from Project to Operations before project closure. Prepare all project documentation such as SOM and status reports, assure report accuracy and timeliness. e.g. Weekly Project Status report, etc. Work collaboratively with technology team and vendors, provide single point of contact and drive resolution of issues that arise in projects. Maintain business partnership with Line of Business (LOBs) and constantly collect and manage user’s demands and applications’ infra requirements. Coordinate with DBA, Solaris, Linux, Windows, Network, ID Mgmt and other Infra admin teams on the system issue related tasks Co-ordinate with various Infra teams to apply the OS / DB / MQ patches  Apply the innovative thinking to automate the manual tasks, provide infrastructure services effortlessly, improve performance and resilience of the systems. "
"VP / AVP, Development Engineer,  Group Consumer Banking & Big Data Analytics Tech, T&O (180001ZC)",DBS BANK LTD.,"Permanent, Full Time",Manager,Banking and Finance,6500.0,13000.0,Monthly," Develop world-class solution/application for your team Be up to date of the market landscape for solution/application insights, direction, vendors, and methods Provides expertise to identify and translate system requirements into software design artefacts Provide input during the business development life cycle Participate in experimentation to assess new solution/application paths Identify challenges to help the development of formalized solution methodologies Contribute to a repository for solution/application artefacts Interface and coordinate tasks with internal and external technical resources. Collaborate to provision estimates, develop overall implementation solution/application plan, and serve as a lead as required, to implement the installation, customization, and integration efforts Actively contribute to the quality assurance for services within the solution/application area Provide relevant and timely project information to senior management Actively contribute to the change in delivery and deployment strategy for all applications to a total replacement for applications at the end of their technology or functionality lifecycle Maintain and monitor all aspects for the proper running of the application Understand the system process flow of the primary business processes. Provide a clear picture of the functionality map and the applications footprint of various applications across the map "," Develop world-class solution/application for your team Be up to date of the market landscape for solution/application insights, direction, vendors, and methods Provides expertise to identify and translate system requirements into software design artefacts Provide input during the business development life cycle Participate in experimentation to assess new solution/application paths Identify challenges to help the development of formalized solution methodologies Contribute to a repository for solution/application artefacts Interface and coordinate tasks with internal and external technical resources. Collaborate to provision estimates, develop overall implementation solution/application plan, and serve as a lead as required, to implement the installation, customization, and integration efforts Actively contribute to the quality assurance for services within the solution/application area Provide relevant and timely project information to senior management Actively contribute to the change in delivery and deployment strategy for all applications to a total replacement for applications at the end of their technology or functionality lifecycle Maintain and monitor all aspects for the proper running of the application Understand the system process flow of the primary business processes. Provide a clear picture of the functionality map and the applications footprint of various applications across the map "
"AVP  /  Senior Associate, Data Analyst, Analytic Center of Excellence, Transformation Grp (180003G5)",DBS BANK LTD.,"Permanent, Full Time","Manager, Senior Executive",Banking and Finance,5500.0,11000.0,Monthly,"Job Purpose The data analyst will provide the big data analytic support to the Analytic Center of Excellence. He will partner with business and project leader to discover, analyse and process the data to develop analytic and data science solutions. This position allows those with strong data analytic skill and theoretical understanding of advanced analytic algorithms but lack of hands on experience in advanced analytics to learn and prepare for the role of data scientist - advanced analytics in the future. Responsibilities   Identify, profile, analyze and present the data discovery output for analytic projects Develop data ingestion pipeline and create the analytic data assets for analytic projects Work with data engineer to enhance the analytic data infrastructure and develop enterprise analytic data mart Perform data wrangling and feature engineering for machine learning Create helper functions to automate frequently encountered wrangling and feature engineering tasks ","Job Purpose The data analyst will provide the big data analytic support to the Analytic Center of Excellence. He will partner with business and project leader to discover, analyse and process the data to develop analytic and data science solutions. This position allows those with strong data analytic skill and theoretical understanding of advanced analytic algorithms but lack of hands on experience in advanced analytics to learn and prepare for the role of data scientist - advanced analytics in the future. Responsibilities   Identify, profile, analyze and present the data discovery output for analytic projects Develop data ingestion pipeline and create the analytic data assets for analytic projects Work with data engineer to enhance the analytic data infrastructure and develop enterprise analytic data mart Perform data wrangling and feature engineering for machine learning Create helper functions to automate frequently encountered wrangling and feature engineering tasks "
DATA ENGINEER,CHARLES & KEITH (SINGAPORE) PTE. LTD.,Full Time,Executive,Information Technology,5000.0,6000.0,Monthly,"We are looking for a savvy Software turned Data Engineer to join our growing Data Engineering team. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. He/She will be responsible for designing, expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. Supporting our software developers, data architect, data analysts and data scientists on organisation wide data initiatives, he/she will ensure optimal data delivery that is consistent throughout ongoing projects. Self-directed and comfortable supporting the data needs of multiple teams, systems and products, he/she will be excited by the prospect of optimizing or even re-designing our organisation’s data architecture with architect to support our next generation of products and data initiatives. Roles & Responsibilities  Create and maintain optimal data pipeline architecture Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, collaborate with infra team to re-designing infrastructure for greater scalability and stability Collaborate with infrastructure team for provisioning required infrastructure for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and ‘big data’ technologies Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data needs Keep our data separated and secure across national boundaries through multiple data centres and AWS regions Create data tools for analytics and data scientist team members that assist them in building and optimizing models which enables us as an innovative industry leader. Work with data and analytics experts to strive for greater functionality in our data systems Build tools from ground up that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics     ","We are looking for a savvy Software turned Data Engineer to join our growing Data Engineering team. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. He/She will be responsible for designing, expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. Supporting our software developers, data architect, data analysts and data scientists on organisation wide data initiatives, he/she will ensure optimal data delivery that is consistent throughout ongoing projects. Self-directed and comfortable supporting the data needs of multiple teams, systems and products, he/she will be excited by the prospect of optimizing or even re-designing our organisation’s data architecture with architect to support our next generation of products and data initiatives. Roles & Responsibilities  Create and maintain optimal data pipeline architecture Assemble large, complex data sets that meet functional / non-functional business requirements Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, collaborate with infra team to re-designing infrastructure for greater scalability and stability Collaborate with infrastructure team for provisioning required infrastructure for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and ‘big data’ technologies Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data needs Keep our data separated and secure across national boundaries through multiple data centres and AWS regions Create data tools for analytics and data scientist team members that assist them in building and optimizing models which enables us as an innovative industry leader. Work with data and analytics experts to strive for greater functionality in our data systems Build tools from ground up that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics     "
Data Engineer (Fin-Tech),MATCHMOVE PAY PTE. LTD.,Full Time,"Manager, Senior Executive",Banking and Finance,3500.0,5000.0,Monthly,"Are you the One?  MatchMove Pay, one of the fastest, award-winning Fin-Tech company, is looking for a couple of experienced Data Engineers to work on in-house data warehouse projects and data modelling.  Job Responsibilities :  Build the data warehouse infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources on AWS technologies. Maintain compliance of the data warehouse with MatchMove data architecture policy. Responsible for data modelling and validation of master data with original data sources. Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability Work with stakeholders including the Executive,  Finance, Product and Engineering teams to assist with data-related technical issues and support their data infrastructure needs. 	  ","Are you the One?  MatchMove Pay, one of the fastest, award-winning Fin-Tech company, is looking for a couple of experienced Data Engineers to work on in-house data warehouse projects and data modelling.  Job Responsibilities :  Build the data warehouse infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources on AWS technologies. Maintain compliance of the data warehouse with MatchMove data architecture policy. Responsible for data modelling and validation of master data with original data sources. Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability Work with stakeholders including the Executive,  Finance, Product and Engineering teams to assist with data-related technical issues and support their data infrastructure needs. 	  "
Data Security Network Engineer,EIRE SYSTEMS SINGAPORE PTE. LTD.,Full Time,Professional,Information Technology,7000.0,9000.0,Monthly,"The role of an Implementation Services Data Security Engineer is to interpret a customer request and then transform that request into a successful project.  This includes project management, solution design, security solution implementation as well as act as Tier 3 escalation technical support when called upon.  The role is required to act as liaisons to engineers in the Client Architecture and Engineering department in ensuring that global engineering standards and guidelines are adhered to.   Responsibilities include: Requirements verification:  Meeting with Business Clients, Systems Integration, and other IT departments to gather technical requirements, business justification for projects Verify technical requirements for large projects Quickly estimate high-level costs and resource requirements  Solution design:  Evaluating technology options and presenting those that best suit customer requirements. Produce the technical design in the format specified by Client standards.  This requires excellent technical writing skills and proficiency with office automation tools.  Many technical designs will be in excess of 100+ pages and integrate various items including Visio Diagrams, Tables, Charts, and links. Ensure Compliance with all Client Standards for Data and Voice Networks. Assist junior engineers with project costing estimates Generally the first to use new solutions provided by Network Technology engineering team    Project Management  Develop a High-level plan at project initialization that will guide the project through the requirements and pre-sales stage Develop a Detailed project plan that will guide the project through the solution design, procurement, and delivery stages. Hold regular meeting with stake holders to track progress and communicate issues, and delegate tasks Provide Project reporting as required by the Client Minimal Grid Guidelines for Complex and Medium Service Requests  Implementation:  Determining implementation time frames and risks - Coordinating and gaining consensus on change windows with change management and other IT teams and LBM’s Submission and tracking of Change Management tickets for implementation Coordinating the installation of equipment and connections with Move/Add/Change team Testing of network connectivity post implementations Ensuring Compliance of all Changes with Client standards Completing the Acceptance into Service (AIS) Process for all projects. Delivering all Projects and Changes “Right the First Time” as most of our SLA for service uptime are 99.999% and greater.  Failure to deliver right the first time has significant financial penalties. Travel to various customer sites within the Asia Pacific theater is required    General Support:  Provide Tier 3 Technical Support during major incidents and troubleshooting assistance for complex long term issues impacting network performance. Provide Technical consultation as requested by the customer Assist with internal and external audit requests   ","The role of an Implementation Services Data Security Engineer is to interpret a customer request and then transform that request into a successful project.  This includes project management, solution design, security solution implementation as well as act as Tier 3 escalation technical support when called upon.  The role is required to act as liaisons to engineers in the Client Architecture and Engineering department in ensuring that global engineering standards and guidelines are adhered to.   Responsibilities include: Requirements verification:  Meeting with Business Clients, Systems Integration, and other IT departments to gather technical requirements, business justification for projects Verify technical requirements for large projects Quickly estimate high-level costs and resource requirements  Solution design:  Evaluating technology options and presenting those that best suit customer requirements. Produce the technical design in the format specified by Client standards.  This requires excellent technical writing skills and proficiency with office automation tools.  Many technical designs will be in excess of 100+ pages and integrate various items including Visio Diagrams, Tables, Charts, and links. Ensure Compliance with all Client Standards for Data and Voice Networks. Assist junior engineers with project costing estimates Generally the first to use new solutions provided by Network Technology engineering team    Project Management  Develop a High-level plan at project initialization that will guide the project through the requirements and pre-sales stage Develop a Detailed project plan that will guide the project through the solution design, procurement, and delivery stages. Hold regular meeting with stake holders to track progress and communicate issues, and delegate tasks Provide Project reporting as required by the Client Minimal Grid Guidelines for Complex and Medium Service Requests  Implementation:  Determining implementation time frames and risks - Coordinating and gaining consensus on change windows with change management and other IT teams and LBM’s Submission and tracking of Change Management tickets for implementation Coordinating the installation of equipment and connections with Move/Add/Change team Testing of network connectivity post implementations Ensuring Compliance of all Changes with Client standards Completing the Acceptance into Service (AIS) Process for all projects. Delivering all Projects and Changes “Right the First Time” as most of our SLA for service uptime are 99.999% and greater.  Failure to deliver right the first time has significant financial penalties. Travel to various customer sites within the Asia Pacific theater is required    General Support:  Provide Tier 3 Technical Support during major incidents and troubleshooting assistance for complex long term issues impacting network performance. Provide Technical consultation as requested by the customer Assist with internal and external audit requests   "
Data Quality Engineer,ALPHATECH BUSINESS SOLUTIONS PTE. LTD.,Permanent,Senior Executive,Information Technology,6000.0,8000.0,Monthly, Evaluate and recommend solutions via data analysis regarding issues related to the improvement of product qua;oty and resolving of customer feedback Apply software and programming abilities to manage and analyse data from a variety of sources , Evaluate and recommend solutions via data analysis regarding issues related to the improvement of product qua;oty and resolving of customer feedback Apply software and programming abilities to manage and analyse data from a variety of sources 
"VP, Lead DevOps Engineer, Group Consumer Banking and Big Data Analytics Technology, T&O (180001X5)",DBS BANK LTD.,Full Time,Senior Management,Engineering,9500.0,15000.0,Monthly,"Business Function  Group Technology and Operations (T&O) enables and empowers the bank with an efficient, nimble and resilient infrastructure through a strategic focus on productivity, quality & control, technology, people capability and innovation. In Group T&O, we manage the majority of the Bank's operational processes and inspire to delight our business partners through our multiple banking delivery channels.   Key Accountabilities  Manage the development of the internal engineering productivity tools and environments. Providing DevOps architecture implementation and operational support Architecture and planning for cloud deployments (Private and Public cloud);  Be an innovative and hands-on DevOps engineer capable of looking at both the technology and strategy around the platform. Future-proofing the technical environments and ensuring extreme high levels of automation, availability, scalability and resilience.  Responsibilities   Manage the development of the internal engineering productivity tools and environments. Manage processes, automation, best practices, documentation. Development and operation of continuous integration and deployment pipelines. Monitoring automation to effectively detect/predict/prevent issues in the environment and code base. Ability to conduct research into software issues and products as required Working with the latest tools and techniques  Hands-on coding and mentoring, usually in a pair programming environment  Working in highly collaborative teams and building quality environments. Ability to effectively prioritize and execute tasks in a high-pressure, fast paced, global environment Knowledge in lots of different open source technologies and configurations. ","Business Function  Group Technology and Operations (T&O) enables and empowers the bank with an efficient, nimble and resilient infrastructure through a strategic focus on productivity, quality & control, technology, people capability and innovation. In Group T&O, we manage the majority of the Bank's operational processes and inspire to delight our business partners through our multiple banking delivery channels.   Key Accountabilities  Manage the development of the internal engineering productivity tools and environments. Providing DevOps architecture implementation and operational support Architecture and planning for cloud deployments (Private and Public cloud);  Be an innovative and hands-on DevOps engineer capable of looking at both the technology and strategy around the platform. Future-proofing the technical environments and ensuring extreme high levels of automation, availability, scalability and resilience.  Responsibilities   Manage the development of the internal engineering productivity tools and environments. Manage processes, automation, best practices, documentation. Development and operation of continuous integration and deployment pipelines. Monitoring automation to effectively detect/predict/prevent issues in the environment and code base. Ability to conduct research into software issues and products as required Working with the latest tools and techniques  Hands-on coding and mentoring, usually in a pair programming environment  Working in highly collaborative teams and building quality environments. Ability to effectively prioritize and execute tasks in a high-pressure, fast paced, global environment Knowledge in lots of different open source technologies and configurations. "
"Vice President, Data Engineering",LAZADA SERVICES SOUTH EAST ASIA PTE. LTD.,Permanent,"Senior Management, Middle Management","Information Technology, Logistics / Supply Chain",13000.0,17000.0,Monthly,"Story of Lazada Group: Launched in 2012, Lazada has grown rapidly to include over 4.900 full-time employees in the region, with eCommerce operations in Indonesia, Malaysia, Philippines, Singapore, Thailand, Vietnam and a sourcing center in Hong Kong that drives cross-border marketplace activities as well as an R&D TechHub in Russia. Revolutionizing the way customers shop in Southeast Asia and perform online transactions across the region, Lazada has reached an online footprint of approximately 9 million unique daily visits to its websites, and the largest Facebook community in Southeast Asia with over 16.5 million fans. Lazada Group owns the biggest and the most efficient technology driven logistics and fulfilment ecosystem in the region – Lazada eLogistics. With 11 own warehouses, 5 sorting centers, 78 last-mile hubs we are ensuring 48 hours delivery of more than 6 million orders every month. Our warehouses cover more than 115 thousands of square miles and it takes less than 2 hours to process every order even during massive sales campaigns. Having our own cross-border operator helps us connect more than 100 million of customers and businesses from all over Asia. Our transportation is driven by our own LEL Express delivery fleet which, together with more than 80 third-party logistics, guarantees high quality 48 hours delivery. All of that would be impossible without sophisticated IT systems, which are being developed and expanded in-house by one of the most experienced and agile tech teams in Southeast Asia! As a Lead Data Engineer in Lazada eLogistics Tech Team, you'll be part of an extremely motivated and experienced group of people. You'll help drive LEL business and be a key contributor. You will also become a mentor for other developers and business members. Does the real-time challenge of dealing with massive datasets (billions of transactions a day) get you excited? If yes, then we would like to speak with you. Lazada eLogistics is using a mix of cutting edge and proven technologies to build new data products that aim to change the E-Logistics landscape. You will be the tech leader of a data engineering team that primarily focuses on productionalizing data pipelines that drive our most critical applications. The tech lead position is the a critical layer that makes sure projects get done. Your daily duties will be:   Drive development of real-time data ingestion pipelines and batch data ingestion pipelines for analysis, machine learning, dashboards, alerts and visualizations.   Drive development of new systems and tools to enable data scientists to consume and analyse data faster and more efficiently.   Design data warehouse and data pipelines ensuring data integrity between systems are maintained.   Architect, build, and launch new data models.   Execute code review of data engineers.   Mentor data engineers in the team.   Convert specs into to working code.   Work and tune data warehousing and data ingest environments.   Script programs and APIs in Python/Go.   Create, monitor and manage low latency ETL and Data pipelines.   Your future benefits will be:    Class ""A” office with the best view on business district of Singapore.   Official employment and relocation coverage.   Medical insurance from the first day.   Comfortable working hours in the office.   Caring and respectful HR team.   Powerful workstations and various software licenses (Mac / Winbook + HD displays to your liking).   Daily snacks, chill-out on Friday and of course high quality coffee.   Personal development system for both specialists and managers.   Choice of hackathons, meetups and other entertainment activities.   Opportunity to become public speaker in technology and take part in industry conferences – for top performers.   Exciting international business travels.   No dress code.  ","Story of Lazada Group: Launched in 2012, Lazada has grown rapidly to include over 4.900 full-time employees in the region, with eCommerce operations in Indonesia, Malaysia, Philippines, Singapore, Thailand, Vietnam and a sourcing center in Hong Kong that drives cross-border marketplace activities as well as an R&D TechHub in Russia. Revolutionizing the way customers shop in Southeast Asia and perform online transactions across the region, Lazada has reached an online footprint of approximately 9 million unique daily visits to its websites, and the largest Facebook community in Southeast Asia with over 16.5 million fans. Lazada Group owns the biggest and the most efficient technology driven logistics and fulfilment ecosystem in the region – Lazada eLogistics. With 11 own warehouses, 5 sorting centers, 78 last-mile hubs we are ensuring 48 hours delivery of more than 6 million orders every month. Our warehouses cover more than 115 thousands of square miles and it takes less than 2 hours to process every order even during massive sales campaigns. Having our own cross-border operator helps us connect more than 100 million of customers and businesses from all over Asia. Our transportation is driven by our own LEL Express delivery fleet which, together with more than 80 third-party logistics, guarantees high quality 48 hours delivery. All of that would be impossible without sophisticated IT systems, which are being developed and expanded in-house by one of the most experienced and agile tech teams in Southeast Asia! As a Lead Data Engineer in Lazada eLogistics Tech Team, you'll be part of an extremely motivated and experienced group of people. You'll help drive LEL business and be a key contributor. You will also become a mentor for other developers and business members. Does the real-time challenge of dealing with massive datasets (billions of transactions a day) get you excited? If yes, then we would like to speak with you. Lazada eLogistics is using a mix of cutting edge and proven technologies to build new data products that aim to change the E-Logistics landscape. You will be the tech leader of a data engineering team that primarily focuses on productionalizing data pipelines that drive our most critical applications. The tech lead position is the a critical layer that makes sure projects get done. Your daily duties will be:   Drive development of real-time data ingestion pipelines and batch data ingestion pipelines for analysis, machine learning, dashboards, alerts and visualizations.   Drive development of new systems and tools to enable data scientists to consume and analyse data faster and more efficiently.   Design data warehouse and data pipelines ensuring data integrity between systems are maintained.   Architect, build, and launch new data models.   Execute code review of data engineers.   Mentor data engineers in the team.   Convert specs into to working code.   Work and tune data warehousing and data ingest environments.   Script programs and APIs in Python/Go.   Create, monitor and manage low latency ETL and Data pipelines.   Your future benefits will be:    Class ""A” office with the best view on business district of Singapore.   Official employment and relocation coverage.   Medical insurance from the first day.   Comfortable working hours in the office.   Caring and respectful HR team.   Powerful workstations and various software licenses (Mac / Winbook + HD displays to your liking).   Daily snacks, chill-out on Friday and of course high quality coffee.   Personal development system for both specialists and managers.   Choice of hackathons, meetups and other entertainment activities.   Opportunity to become public speaker in technology and take part in industry conferences – for top performers.   Exciting international business travels.   No dress code.  "
Data Engineer (Big Data),Company Undisclosed,Full Time,Executive,"Engineering, Information Technology, Others",6000.0,8000.0,Monthly," Being part of the Data Engineering Team to conceptualise, build and maintain the Data Infrastructure for relational as well as Big Data Infrastructure in order to deliver a high performanc Data Environment to process Retail and Shopper Data from various partners. Being responsible for Data Warehousing Design, Data Integration Processes, Database Ressource Planning, Infrastructure Performance, Data Governance and Security Management as well as design, architecture, implementation and documentation of new, scalable ETL processes, pipelines, pathways and dimensional data models. Audit and QA data and processes to ensure data quality and integrity throughout the data ecosystem Work with Data Scientist, Business Consultant and Frontend Developer to build scalable Data Analytics Apps using advanced analytics.     "," Being part of the Data Engineering Team to conceptualise, build and maintain the Data Infrastructure for relational as well as Big Data Infrastructure in order to deliver a high performanc Data Environment to process Retail and Shopper Data from various partners. Being responsible for Data Warehousing Design, Data Integration Processes, Database Ressource Planning, Infrastructure Performance, Data Governance and Security Management as well as design, architecture, implementation and documentation of new, scalable ETL processes, pipelines, pathways and dimensional data models. Audit and QA data and processes to ensure data quality and integrity throughout the data ecosystem Work with Data Scientist, Business Consultant and Frontend Developer to build scalable Data Analytics Apps using advanced analytics.     "
Data Engineer,ZALORA SOUTH EAST ASIA PTE. LTD.,Full Time,Professional,Engineering,4500.0,6000.0,Monthly,"We are looking for a Big Data Engineer happy to design, build, maintain and automate big data environments (datalake, etc…) and the associated data to enable the teams to make use of the high volume of data available from our e-commerce activities. You should be proficient in: - Technical background:  Linux Big Data technologies (Redshift, BigQuery, Spark, Glue, Parquet…) Industrialization (Ansible..), Orchestration (Kubernetes…), containers in cloud (Docker, AWS…) Strong experience in resilient architecture (high availability, scalability) Data management: you have experience in integrating and managing large volumes of Data while taking into account performance issues Coding skills: skills in one or more scripting languages (Perl, Ruby…) as well as one or more development languages (Python, Java…)  Soft skills: while being a tech automation enthusiast with a passion for building tools to make developers' lives easier, you also want and know how to share your expertise with other people to empower them. Agile and DevOps approach, with an operational experience as an Ops in a demanding environment. You know what it’s like to manage in production critical systems and you have experience in sharing this knowledge to the teams to enable a “you build it / you run it” mindset.","We are looking for a Big Data Engineer happy to design, build, maintain and automate big data environments (datalake, etc…) and the associated data to enable the teams to make use of the high volume of data available from our e-commerce activities. You should be proficient in: - Technical background:  Linux Big Data technologies (Redshift, BigQuery, Spark, Glue, Parquet…) Industrialization (Ansible..), Orchestration (Kubernetes…), containers in cloud (Docker, AWS…) Strong experience in resilient architecture (high availability, scalability) Data management: you have experience in integrating and managing large volumes of Data while taking into account performance issues Coding skills: skills in one or more scripting languages (Perl, Ruby…) as well as one or more development languages (Python, Java…)  Soft skills: while being a tech automation enthusiast with a passion for building tools to make developers' lives easier, you also want and know how to share your expertise with other people to empower them. Agile and DevOps approach, with an operational experience as an Ops in a demanding environment. You know what it’s like to manage in production critical systems and you have experience in sharing this knowledge to the teams to enable a “you build it / you run it” mindset."
senior Big data Engineer,SMARTSOFT PTE. LTD.,Full Time,Senior Executive,Information Technology,4000.0,8000.0,Monthly," Massive data: You will source / examine, analyze, engineer data pipelines for gigabytes/terabytes of structured and unstructured data with our platform to create value for customers. Pushing the limits: This role will be on the cutting edge of our Data / Machine Learning platform. As we push to solve more of our customer challenges, you will be prototyping new features, tools and ideas. Innovate at a very fast pace to maintain our competitive edge. Linux hacking: You will be masterfully using the command line, including tools like vi/emacs and understanding beyond basics of grep, bash, awk, sed, etc to aggressively dive into data, systems, and compute platforms to get the results you are seeking. Production deployment: You will be responsible for integration and deployment of the machine learning pipelines into production where your ideas can come to life. Coordinate and work with cross functional teams, sometimes located at different geo locations. "," Massive data: You will source / examine, analyze, engineer data pipelines for gigabytes/terabytes of structured and unstructured data with our platform to create value for customers. Pushing the limits: This role will be on the cutting edge of our Data / Machine Learning platform. As we push to solve more of our customer challenges, you will be prototyping new features, tools and ideas. Innovate at a very fast pace to maintain our competitive edge. Linux hacking: You will be masterfully using the command line, including tools like vi/emacs and understanding beyond basics of grep, bash, awk, sed, etc to aggressively dive into data, systems, and compute platforms to get the results you are seeking. Production deployment: You will be responsible for integration and deployment of the machine learning pipelines into production where your ideas can come to life. Coordinate and work with cross functional teams, sometimes located at different geo locations. "
Data Engineer - SQL  /  Big Data  /  Java,DENODO TECHNOLOGIES PTE. LTD.,Full Time,Professional,Engineering,,,,"Your Opportunity Denodo is always looking for technical, passionate people to join our Services Engineering team. We want a professional who will travel, consult, develop, train and troubleshoot to enhance our clients’ journey around Data Virtualization. Your mission: to help people realize their full potential through accelerated adoption and productive use of Denodo solutions. In this role you will successfully employ a combination of high technical expertise and client management skills to conduct on-site and off-site consulting, product implementation and solutions development in either short or long-term engagements being critical point of contact for getting things done among Denodo, partners and client teams. Duties & Responsibilities  Obtain and maintain strong knowledge of the Denodo Platform, be able to deliver a superb technical pitch, including overview of our key and advanced features and benefits, services offerings, differentiation, and competitive positioning. Constantly learn new things and maintain an overview of modern technologies. Be able to address a majority of technical questions concerning customization, integration, enterprise architecture and general feature / functionality of our product. Capable of building and/or leading the development of custom deployments based and beyond client’s requirements. Provide timely, prioritized and complete customer-based feedback to Product Management, Sales, Support and/or Development regarding client’s business cases, requirements and issues. Train and engage clients in the product architecture, configuration, and use of the Denodo Platform. Promote knowledge and best practices while managing deliverables and client expectations. Manage client expectations, establish credibility at all levels within the client and build problem-solving partnerships with the client, partners and colleagues. Provide technical consulting, training and support. Develop white papers, presentations, training materials or documentation on related topics ","Your Opportunity Denodo is always looking for technical, passionate people to join our Services Engineering team. We want a professional who will travel, consult, develop, train and troubleshoot to enhance our clients’ journey around Data Virtualization. Your mission: to help people realize their full potential through accelerated adoption and productive use of Denodo solutions. In this role you will successfully employ a combination of high technical expertise and client management skills to conduct on-site and off-site consulting, product implementation and solutions development in either short or long-term engagements being critical point of contact for getting things done among Denodo, partners and client teams. Duties & Responsibilities  Obtain and maintain strong knowledge of the Denodo Platform, be able to deliver a superb technical pitch, including overview of our key and advanced features and benefits, services offerings, differentiation, and competitive positioning. Constantly learn new things and maintain an overview of modern technologies. Be able to address a majority of technical questions concerning customization, integration, enterprise architecture and general feature / functionality of our product. Capable of building and/or leading the development of custom deployments based and beyond client’s requirements. Provide timely, prioritized and complete customer-based feedback to Product Management, Sales, Support and/or Development regarding client’s business cases, requirements and issues. Train and engage clients in the product architecture, configuration, and use of the Denodo Platform. Promote knowledge and best practices while managing deliverables and client expectations. Manage client expectations, establish credibility at all levels within the client and build problem-solving partnerships with the client, partners and colleagues. Provide technical consulting, training and support. Develop white papers, presentations, training materials or documentation on related topics "
Senior Database Consultant - Big Data Engineer,PALO IT SINGAPORE PTE. LTD.,"Permanent, Full Time",Professional,Information Technology,6000.0,12000.0,Monthly,"Your profile & role on the project YOU:  Thrive on challenge. When was the last time you failed? Are curious & always learning. What are you up to right now? Can deal with constant change. When were you last surprised? Have mastered at least one skill of your trade but you’re not defined by it. What can you teach us? Can you wear many hats?  YOU AGAIN: The DevOps Architect will install, maintain, and support an on-premises cloud infrastructure and apply DevOps practices and solutions. The person will also implement cloud-related and DevOps technologies such as AWS/Puppet/Chef/Elk/Azure/Openstack. Other infrastructure related activities such as maintaining the company internal server infrastructure and respond to consultant requests when required will be expected.  Install, maintain, and support on-premises and off-premises cloud stack. Configure, maintain, and support the cloud-related infrastructures. Act as a system administrator on different OSes (e.g. RHEL, Opensolaris, Ubuntu, etc.) and help teams deploy their application and automate their development and releases on the cloud. Ability to develop solutions and self-learn new tools and technologies. Document, and share knowledge on developed DevOps solutions.  STILL YOU:  Unix / Linux / Bash knowledge Very good understanding of cloud computing (e.g. Technologies, Deployment, costing, HA/DR, etc.) Good understanding of DevOps principles (e.g. testing automation, BDD, TDD, Release automation, CI/CD, etc.) 2 years experience with cloud deployment (e.g. Openstack, VMWare, AWS, Azure, Terraform, etc.) 1 year experience with testing automation (e.g. Maven, Selenium, HP QC, LoadRunner) 1 year experience with release automation process (e.g. CA-RA, Jenkins, etc.) 1 year experience with Configuration Management (e.g. Ansible, SaltStack, Puppet/Chef, etc.) 1 year experience with monitoring tools (e.g. ELK, Prometheus, Grafana and Splunk) Experience with developing and implementing processes to handle releases from Development to Operations while respecting internal rules, and offering solutions for rollback) Experience with designing an architecture to implement development-to-production workflows. Knowledge of SRE, Containers, Kubernetes, Openshift is a plus. Good understanding of microservice architecture and DevOps practices that support. Strong RDMS and NoSQL skill in deploying and fine tuning such as MySQL, Oracle, Elasticsearch.  Your role at PALO IT You will be invited to take part in R&D works done within our Practices. You will have the chance to assist or be a speaker at must-attend international IT conferences. You will have the opportunity to write articles for our Blog or specialized press. Genuine ambassador of PALO IT, you will present our offers and take an active role in the development of the company.  Your technical environment # Cloud and DevOps based technologies (AWS/Puppet/Chef/Elk/Azure/Opencloud) # DevOps practices # Linux OS, Shell Scripting, SQL # Agile and scrum environment","Your profile & role on the project YOU:  Thrive on challenge. When was the last time you failed? Are curious & always learning. What are you up to right now? Can deal with constant change. When were you last surprised? Have mastered at least one skill of your trade but you’re not defined by it. What can you teach us? Can you wear many hats?  YOU AGAIN: The DevOps Architect will install, maintain, and support an on-premises cloud infrastructure and apply DevOps practices and solutions. The person will also implement cloud-related and DevOps technologies such as AWS/Puppet/Chef/Elk/Azure/Openstack. Other infrastructure related activities such as maintaining the company internal server infrastructure and respond to consultant requests when required will be expected.  Install, maintain, and support on-premises and off-premises cloud stack. Configure, maintain, and support the cloud-related infrastructures. Act as a system administrator on different OSes (e.g. RHEL, Opensolaris, Ubuntu, etc.) and help teams deploy their application and automate their development and releases on the cloud. Ability to develop solutions and self-learn new tools and technologies. Document, and share knowledge on developed DevOps solutions.  STILL YOU:  Unix / Linux / Bash knowledge Very good understanding of cloud computing (e.g. Technologies, Deployment, costing, HA/DR, etc.) Good understanding of DevOps principles (e.g. testing automation, BDD, TDD, Release automation, CI/CD, etc.) 2 years experience with cloud deployment (e.g. Openstack, VMWare, AWS, Azure, Terraform, etc.) 1 year experience with testing automation (e.g. Maven, Selenium, HP QC, LoadRunner) 1 year experience with release automation process (e.g. CA-RA, Jenkins, etc.) 1 year experience with Configuration Management (e.g. Ansible, SaltStack, Puppet/Chef, etc.) 1 year experience with monitoring tools (e.g. ELK, Prometheus, Grafana and Splunk) Experience with developing and implementing processes to handle releases from Development to Operations while respecting internal rules, and offering solutions for rollback) Experience with designing an architecture to implement development-to-production workflows. Knowledge of SRE, Containers, Kubernetes, Openshift is a plus. Good understanding of microservice architecture and DevOps practices that support. Strong RDMS and NoSQL skill in deploying and fine tuning such as MySQL, Oracle, Elasticsearch.  Your role at PALO IT You will be invited to take part in R&D works done within our Practices. You will have the chance to assist or be a speaker at must-attend international IT conferences. You will have the opportunity to write articles for our Blog or specialized press. Genuine ambassador of PALO IT, you will present our offers and take an active role in the development of the company.  Your technical environment # Cloud and DevOps based technologies (AWS/Puppet/Chef/Elk/Azure/Opencloud) # DevOps practices # Linux OS, Shell Scripting, SQL # Agile and scrum environment"
"Manager, Data Engineer",AVIVA ASIA PTE LTD,Permanent,Manager,Information Technology,10000.0,12000.0,Monthly,"PURPOSE AND CONTEXT OF THE ROLE Taking leadership in:  Design and develop architecture for data services ecosystem spanning Relational and Big Data technologies Design data models for mission critical and high volume data management, real-time and distributed data process aligning with the business requirements. Work with business units on their analytics initiatives, supply and/or source analytics expertise and resources. Ensure the appropriate technology resourcing and support for the Data Engineering and analytics team.    Lead projects involving high level of coordination among departments and business areas.   OUTCOMES  Produce optimal solutions in the receipt and delivery of data sets to desired destinations Alignment to Data Strategy, Digital Strategy, IT Strategy, Architecture and Transformation roadmap Maintain IT Applications Development Excellence amongst IT Development community through detailed development and training and the conduct of Knowledge transfer for completed tasks Produce Optimal solutions, through detailed Impact Analysis, Technical Solutions, Technical Specifications and provide Leadership in Projects, Change Initiatives and Solution Delivery. Supervise IT teams to produce and complete assigned tasks for Strategic, Mandatory and Tactical Change Request, within budget and within projected Scope of work.   ","PURPOSE AND CONTEXT OF THE ROLE Taking leadership in:  Design and develop architecture for data services ecosystem spanning Relational and Big Data technologies Design data models for mission critical and high volume data management, real-time and distributed data process aligning with the business requirements. Work with business units on their analytics initiatives, supply and/or source analytics expertise and resources. Ensure the appropriate technology resourcing and support for the Data Engineering and analytics team.    Lead projects involving high level of coordination among departments and business areas.   OUTCOMES  Produce optimal solutions in the receipt and delivery of data sets to desired destinations Alignment to Data Strategy, Digital Strategy, IT Strategy, Architecture and Transformation roadmap Maintain IT Applications Development Excellence amongst IT Development community through detailed development and training and the conduct of Knowledge transfer for completed tasks Produce Optimal solutions, through detailed Impact Analysis, Technical Solutions, Technical Specifications and provide Leadership in Projects, Change Initiatives and Solution Delivery. Supervise IT teams to produce and complete assigned tasks for Strategic, Mandatory and Tactical Change Request, within budget and within projected Scope of work.   "
senior data engineer,FINSURGE PTE. LTD.,Full Time,Senior Management,Information Technology,7000.0,8500.0,Monthly,"We are looking for a Senior Data Engineer who will join the Digital Technology Team as we are at an early developmental stage and planning for considerable growth over the next 12 months. We need an experienced data engineer to design and develop data infrastructure. As we are looking to build the data pipeline from scratch, you will have autonomy and the technical backing from our engineering team in designing,developing and maintaining this infrastructure. Contribute to key data pipeline architecture decisions and lead the implementation of major initiatives Translating business requirements into technical specifications and documentation Developing code standards, ETL architecture standards, and naming conventions Designing, executing and documenting ETL testing plans Optimizing performance of ETL jobs Develop the team’s data capabilities - share knowledge, enforce best practices and encourage data-driven decisions.","We are looking for a Senior Data Engineer who will join the Digital Technology Team as we are at an early developmental stage and planning for considerable growth over the next 12 months. We need an experienced data engineer to design and develop data infrastructure. As we are looking to build the data pipeline from scratch, you will have autonomy and the technical backing from our engineering team in designing,developing and maintaining this infrastructure. Contribute to key data pipeline architecture decisions and lead the implementation of major initiatives Translating business requirements into technical specifications and documentation Developing code standards, ETL architecture standards, and naming conventions Designing, executing and documenting ETL testing plans Optimizing performance of ETL jobs Develop the team’s data capabilities - share knowledge, enforce best practices and encourage data-driven decisions."
"Senior Manager, Data Engineer",LAZADA SOUTH EAST ASIA PTE. LTD.,Permanent,"Middle Management, Manager",Information Technology,7000.0,10500.0,Monthly,"Team Introduction Lazada is the number one online shopping & selling destination in Southeast Asia – present in Indonesia, Malaysia, the Philippines, Singapore, Thailand and Vietnam. Lazada helps more than 80,000 local and international sellers as well as 2,500 brands serve the 560 million consumers in the region through its marketplace platform, supported by a wide range of tailored marketing, data, and service solutions. Lazada offers an excellent customer experience through a wide network of logistics partners and its own first- and last-mile delivery arm. Roles & Responsibilities  Implementing working data projects based on given technical specifications Develop real-time and batch data ingesting and processing pipelines to be used for analysis, machine learning, dashboards, alerts and visualizations. Work closely with partner teams to establish the optimal technical solution to business problems Monitor & manage data pipelines, ensuring accuracy and stability ","Team Introduction Lazada is the number one online shopping & selling destination in Southeast Asia – present in Indonesia, Malaysia, the Philippines, Singapore, Thailand and Vietnam. Lazada helps more than 80,000 local and international sellers as well as 2,500 brands serve the 560 million consumers in the region through its marketplace platform, supported by a wide range of tailored marketing, data, and service solutions. Lazada offers an excellent customer experience through a wide network of logistics partners and its own first- and last-mile delivery arm. Roles & Responsibilities  Implementing working data projects based on given technical specifications Develop real-time and batch data ingesting and processing pipelines to be used for analysis, machine learning, dashboards, alerts and visualizations. Work closely with partner teams to establish the optimal technical solution to business problems Monitor & manage data pipelines, ensuring accuracy and stability "
Data Engineer,ABAKUS (ASIA PACIFIC) PTE. LTD.,Permanent,"Executive, Senior Executive","Banking and Finance, Information Technology",4000.0,6500.0,Monthly,"To reinvent an industry, you need to build an all-star team. Join Wecash if you want to leverage upon the power of big data and machine learning to develop and promote products that can provide businesses with better credit profiles of customers and underwrite loans between funding sources and consumers. Founded in 2014, we are the first Chinese startup using big data and machine learning to evaluate consumer credit and detect fraud. Our company has raised more than US$200 million in financing over 4 rounds, acquired over 130 million users and have underwritten over Billions of USD loans to transform the lifestyle and credit worthiness of individuals over the past 4 years. We are looking for a stellar technologist to drive our expansion in South Asia. If you can understand complex technology, navigate the fintech industry and thrive under ambiguous objectives, join us as our Senior/Lead Data Engineer for Southeast Asia, and help us grow.","To reinvent an industry, you need to build an all-star team. Join Wecash if you want to leverage upon the power of big data and machine learning to develop and promote products that can provide businesses with better credit profiles of customers and underwrite loans between funding sources and consumers. Founded in 2014, we are the first Chinese startup using big data and machine learning to evaluate consumer credit and detect fraud. Our company has raised more than US$200 million in financing over 4 rounds, acquired over 130 million users and have underwritten over Billions of USD loans to transform the lifestyle and credit worthiness of individuals over the past 4 years. We are looking for a stellar technologist to drive our expansion in South Asia. If you can understand complex technology, navigate the fintech industry and thrive under ambiguous objectives, join us as our Senior/Lead Data Engineer for Southeast Asia, and help us grow."
"Software Engineer, Data Services",HELIX LEISURE PTE. LTD.,Permanent,Senior Executive,Information Technology,5000.0,10000.0,Monthly,"Helix Leisure is a leading global supplier to the Out of Home Entertainment industry – locations outside the home people visit for entertainment and recreation. Across our core brands – Embed (revenue management systems, e-commerce), Booking Boss (Tours, Attractions and Activities), LAI Games (arcade games), The Locker Network (operating electronic lockers) and Matahari Leisure (equipment manufacturing) we service over 2,500 locations around the globe. Helix operates full service offices in Singapore, Perth, Sydney, Dallas, Dubai and Jakarta. The group enables our customers to create rich experiences for their visitors and guests through both technology and service. As we embark on building the next generation platform for our software – a core consumer, supplier and distributor facing application, we are looking for highly motivated professionals who enjoy working in a fast paced, agile development environment. You will be working closely with product owners and UX designers to create and develop best-in-class data service solutions with the ability to use the latest in web development technology. Responsibilities:  Design, develop, test, deploy, maintain and improve software Manage individual project priorities, deadlines and deliverables. ","Helix Leisure is a leading global supplier to the Out of Home Entertainment industry – locations outside the home people visit for entertainment and recreation. Across our core brands – Embed (revenue management systems, e-commerce), Booking Boss (Tours, Attractions and Activities), LAI Games (arcade games), The Locker Network (operating electronic lockers) and Matahari Leisure (equipment manufacturing) we service over 2,500 locations around the globe. Helix operates full service offices in Singapore, Perth, Sydney, Dallas, Dubai and Jakarta. The group enables our customers to create rich experiences for their visitors and guests through both technology and service. As we embark on building the next generation platform for our software – a core consumer, supplier and distributor facing application, we are looking for highly motivated professionals who enjoy working in a fast paced, agile development environment. You will be working closely with product owners and UX designers to create and develop best-in-class data service solutions with the ability to use the latest in web development technology. Responsibilities:  Design, develop, test, deploy, maintain and improve software Manage individual project priorities, deadlines and deliverables. "
Research Engineer (Data Analytics)  /  I2R (A*STAR),A*STAR RESEARCH ENTITIES,"Contract, Full Time",Professional,Sciences / Laboratory / R&D,2500.0,5000.0,Monthly,"About the Institute for Infocomm Research (I²R) The Institute for Infocomm Research (I²R) is a member of the Agency for Science, Technology and Research (A*STAR) family and is Singapore’s largest ICT research institute. Our strategic thrusts are in the spheres of intelligence, communications and media and our research capabilities are in shared sensor networks, public-public/public-private data-sharing platform, big data analytics and visualization solutions. For more information about I2R, please visit www.i2r.a-star.edu.sg We are looking for highly-motivated and skilled data engineer to work on a new initiative on developing advanced automatic data pre-processing techniques for facilitating key data analytics applications in various industry domains, including advanced manufacturing and engineering, financial services, healthcare, and urban development. Successful candidate will work with a team of data scientists and data engineers to develop novel methodology for automatic data integrity check, data imputation, and segmentation for structured data and time-series data commonly generated by industry companies. Successful candidate will have the opportunity to tap into a large pool of industrial data from different disciplines and to interact with the companies to understand their real data needs. Successful candidate will also engage in project scoping with industry companies to develop solution to address the data analytics needs.  ","About the Institute for Infocomm Research (I²R) The Institute for Infocomm Research (I²R) is a member of the Agency for Science, Technology and Research (A*STAR) family and is Singapore’s largest ICT research institute. Our strategic thrusts are in the spheres of intelligence, communications and media and our research capabilities are in shared sensor networks, public-public/public-private data-sharing platform, big data analytics and visualization solutions. For more information about I2R, please visit www.i2r.a-star.edu.sg We are looking for highly-motivated and skilled data engineer to work on a new initiative on developing advanced automatic data pre-processing techniques for facilitating key data analytics applications in various industry domains, including advanced manufacturing and engineering, financial services, healthcare, and urban development. Successful candidate will work with a team of data scientists and data engineers to develop novel methodology for automatic data integrity check, data imputation, and segmentation for structured data and time-series data commonly generated by industry companies. Successful candidate will have the opportunity to tap into a large pool of industrial data from different disciplines and to interact with the companies to understand their real data needs. Successful candidate will also engage in project scoping with industry companies to develop solution to address the data analytics needs.  "
Research Engineer (Data Analytics)  /  I2R (A*STAR),A*STAR RESEARCH ENTITIES,"Contract, Full Time",Professional,Sciences / Laboratory / R&D,2500.0,5000.0,Monthly,"About the Institute for Infocomm Research (I²R) The Institute for Infocomm Research (I²R) is a member of the Agency for Science, Technology and Research (A*STAR) family and is Singapore’s largest ICT research institute. Our strategic thrusts are in the spheres of intelligence, communications and media and our research capabilities are in shared sensor networks, public-public/public-private data-sharing platform, big data analytics and visualization solutions. For more information about I2R, please visit www.i2r.a-star.edu.sg We are looking for highly-motivated and skilled data engineer to work on a new initiative on developing advanced automatic data pre-processing techniques for facilitating key data analytics applications in various industry domains, including advanced manufacturing and engineering, financial services, healthcare, and urban development. Successful candidate will work with a team of data scientists and data engineers to develop novel methodology for automatic data integrity check, data imputation, and segmentation for structured data and time-series data commonly generated by industry companies. Successful candidate will have the opportunity to tap into a large pool of industrial data from different disciplines and to interact with the companies to understand their real data needs. Successful candidate will also engage in project scoping with industry companies to develop solution to address the data analytics needs.  ","About the Institute for Infocomm Research (I²R) The Institute for Infocomm Research (I²R) is a member of the Agency for Science, Technology and Research (A*STAR) family and is Singapore’s largest ICT research institute. Our strategic thrusts are in the spheres of intelligence, communications and media and our research capabilities are in shared sensor networks, public-public/public-private data-sharing platform, big data analytics and visualization solutions. For more information about I2R, please visit www.i2r.a-star.edu.sg We are looking for highly-motivated and skilled data engineer to work on a new initiative on developing advanced automatic data pre-processing techniques for facilitating key data analytics applications in various industry domains, including advanced manufacturing and engineering, financial services, healthcare, and urban development. Successful candidate will work with a team of data scientists and data engineers to develop novel methodology for automatic data integrity check, data imputation, and segmentation for structured data and time-series data commonly generated by industry companies. Successful candidate will have the opportunity to tap into a large pool of industrial data from different disciplines and to interact with the companies to understand their real data needs. Successful candidate will also engage in project scoping with industry companies to develop solution to address the data analytics needs.  "
Research Engineer (Data Analytics)  /  I2R (A*STAR),A*STAR RESEARCH ENTITIES,"Contract, Full Time",Professional,Sciences / Laboratory / R&D,2500.0,5000.0,Monthly,"About the Institute for Infocomm Research (I²R) The Institute for Infocomm Research (I²R) is a member of the Agency for Science, Technology and Research (A*STAR) family and is Singapore’s largest ICT research institute. Our strategic thrusts are in the spheres of intelligence, communications and media and our research capabilities are in shared sensor networks, public-public/public-private data-sharing platform, big data analytics and visualization solutions. For more information about I2R, please visit www.i2r.a-star.edu.sg We are looking for highly-motivated and skilled data engineer to work on a new initiative on developing advanced automatic data pre-processing techniques for facilitating key data analytics applications in various industry domains, including advanced manufacturing and engineering, financial services, healthcare, and urban development. Successful candidate will work with a team of data scientists and data engineers to develop novel methodology for automatic data integrity check, data imputation, and segmentation for structured data and time-series data commonly generated by industry companies. Successful candidate will have the opportunity to tap into a large pool of industrial data from different disciplines and to interact with the companies to understand their real data needs. Successful candidate will also engage in project scoping with industry companies to develop solution to address the data analytics needs.  ","About the Institute for Infocomm Research (I²R) The Institute for Infocomm Research (I²R) is a member of the Agency for Science, Technology and Research (A*STAR) family and is Singapore’s largest ICT research institute. Our strategic thrusts are in the spheres of intelligence, communications and media and our research capabilities are in shared sensor networks, public-public/public-private data-sharing platform, big data analytics and visualization solutions. For more information about I2R, please visit www.i2r.a-star.edu.sg We are looking for highly-motivated and skilled data engineer to work on a new initiative on developing advanced automatic data pre-processing techniques for facilitating key data analytics applications in various industry domains, including advanced manufacturing and engineering, financial services, healthcare, and urban development. Successful candidate will work with a team of data scientists and data engineers to develop novel methodology for automatic data integrity check, data imputation, and segmentation for structured data and time-series data commonly generated by industry companies. Successful candidate will have the opportunity to tap into a large pool of industrial data from different disciplines and to interact with the companies to understand their real data needs. Successful candidate will also engage in project scoping with industry companies to develop solution to address the data analytics needs.  "
Research Engineer (Data Analytics)  /  I2R (A*STAR),A*STAR RESEARCH ENTITIES,"Contract, Full Time",Professional,Sciences / Laboratory / R&D,2500.0,5000.0,Monthly,"About the Institute for Infocomm Research (I²R) The Institute for Infocomm Research (I²R) is a member of the Agency for Science, Technology and Research (A*STAR) family and is Singapore’s largest ICT research institute. Our strategic thrusts are in the spheres of intelligence, communications and media and our research capabilities are in shared sensor networks, public-public/public-private data-sharing platform, big data analytics and visualization solutions. For more information about I2R, please visit www.i2r.a-star.edu.sg We are looking for highly-motivated and skilled data engineer to work on a new initiative on developing advanced automatic data pre-processing techniques for facilitating key data analytics applications in various industry domains, including advanced manufacturing and engineering, financial services, healthcare, and urban development. Successful candidate will work with a team of data scientists and data engineers to develop novel methodology for automatic data integrity check, data imputation, and segmentation for structured data and time-series data commonly generated by industry companies. Successful candidate will have the opportunity to tap into a large pool of industrial data from different disciplines and to interact with the companies to understand their real data needs. Successful candidate will also engage in project scoping with industry companies to develop solution to address the data analytics needs.  ","About the Institute for Infocomm Research (I²R) The Institute for Infocomm Research (I²R) is a member of the Agency for Science, Technology and Research (A*STAR) family and is Singapore’s largest ICT research institute. Our strategic thrusts are in the spheres of intelligence, communications and media and our research capabilities are in shared sensor networks, public-public/public-private data-sharing platform, big data analytics and visualization solutions. For more information about I2R, please visit www.i2r.a-star.edu.sg We are looking for highly-motivated and skilled data engineer to work on a new initiative on developing advanced automatic data pre-processing techniques for facilitating key data analytics applications in various industry domains, including advanced manufacturing and engineering, financial services, healthcare, and urban development. Successful candidate will work with a team of data scientists and data engineers to develop novel methodology for automatic data integrity check, data imputation, and segmentation for structured data and time-series data commonly generated by industry companies. Successful candidate will have the opportunity to tap into a large pool of industrial data from different disciplines and to interact with the companies to understand their real data needs. Successful candidate will also engage in project scoping with industry companies to develop solution to address the data analytics needs.  "
Senior Research Engineer (Data Analytics)  /  I2R (A*STAR),A*STAR RESEARCH ENTITIES,"Contract, Full Time",Professional,Sciences / Laboratory / R&D,3400.0,6800.0,Monthly,"About the Institute for Infocomm Research (I²R) The Institute for Infocomm Research (I²R) is a member of the Agency for Science, Technology and Research (A*STAR) family and is Singapore’s largest ICT research institute. Our strategic thrusts are in the spheres of intelligence, communications and media and our research capabilities are in shared sensor networks, public-public/public-private data-sharing platform, big data analytics and visualization solutions. For more information about I2R, please visit www.i2r.a-star.edu.sg The project focuses on data management systems, data engineering solutions and deep learning systems for radiology applications. Candidate should have demonstrated interests or experience in: 1. Experience in the execution of translational projects focused on development, testing and deployment  2. Big data analytics and data engineering 3. Experience with biomedical datasets, in particular medical images 4. Business analysis skills and/or past work with/in clinical partner institutions  The position entails working in a multi-disciplinary business analytics translation group alongside machine learning and deep learning teams that are closely collaborating with clinical and industry partners on impactful projects that will translate research to deployed technology.","About the Institute for Infocomm Research (I²R) The Institute for Infocomm Research (I²R) is a member of the Agency for Science, Technology and Research (A*STAR) family and is Singapore’s largest ICT research institute. Our strategic thrusts are in the spheres of intelligence, communications and media and our research capabilities are in shared sensor networks, public-public/public-private data-sharing platform, big data analytics and visualization solutions. For more information about I2R, please visit www.i2r.a-star.edu.sg The project focuses on data management systems, data engineering solutions and deep learning systems for radiology applications. Candidate should have demonstrated interests or experience in: 1. Experience in the execution of translational projects focused on development, testing and deployment  2. Big data analytics and data engineering 3. Experience with biomedical datasets, in particular medical images 4. Business analysis skills and/or past work with/in clinical partner institutions  The position entails working in a multi-disciplinary business analytics translation group alongside machine learning and deep learning teams that are closely collaborating with clinical and industry partners on impactful projects that will translate research to deployed technology."
Research Engineer (Data Analytics)  /  I2R (A*STAR),A*STAR RESEARCH ENTITIES,"Contract, Full Time",Professional,Sciences / Laboratory / R&D,2500.0,5000.0,Monthly,"About the Institute for Infocomm Research (I²R)  The Institute for Infocomm Research (I²R) is a member of the Agency for Science, Technology and Research (A*STAR) family and is Singapore’s largest ICT research institute. Our strategic thrusts are in the spheres of intelligence, communications and media and our research capabilities are in shared sensor networks, public-public/public-private data-sharing platform, big data analytics and visualisation solutions. For more information about I²R, please visit www.i2r.a-star.edu.sg We are looking for a capable and responsible engineer to work on and make contributions to a major big data R&D project on fraud risk prediction. The work scope involves design and implementation of big data management, analytics and web applications. Successful candidate will work with other research team members to do part of system implementation, data pre-processing, analysis and visualization. Successful candidate will also have opportunities to be involved in other industry projects and/or research projects.  ","About the Institute for Infocomm Research (I²R)  The Institute for Infocomm Research (I²R) is a member of the Agency for Science, Technology and Research (A*STAR) family and is Singapore’s largest ICT research institute. Our strategic thrusts are in the spheres of intelligence, communications and media and our research capabilities are in shared sensor networks, public-public/public-private data-sharing platform, big data analytics and visualisation solutions. For more information about I²R, please visit www.i2r.a-star.edu.sg We are looking for a capable and responsible engineer to work on and make contributions to a major big data R&D project on fraud risk prediction. The work scope involves design and implementation of big data management, analytics and web applications. Successful candidate will work with other research team members to do part of system implementation, data pre-processing, analysis and visualization. Successful candidate will also have opportunities to be involved in other industry projects and/or research projects.  "
Research Engineer (Data Analytics)  /  I2R (A*STAR),A*STAR RESEARCH ENTITIES,"Contract, Full Time",Professional,Sciences / Laboratory / R&D,2500.0,5000.0,Monthly,"About the Institute for Infocomm Research (I²R)  The Institute for Infocomm Research (I²R) is a member of the Agency for Science, Technology and Research (A*STAR) family and is Singapore’s largest ICT research institute. Our strategic thrusts are in the spheres of intelligence, communications and media and our research capabilities are in shared sensor networks, public-public/public-private data-sharing platform, big data analytics and visualisation solutions. For more information about I²R, please visit www.i2r.a-star.edu.sg The project focuses on development of machine learning, deep learning and artificial intelligence algorithms for applicants in precision medicine. Candidates should have demonstrated interests or experience in one or more of the following:  Analysis of large scale heterogeneous biomedical datastreams (genomics, EMR, imaging, lifestyle) Projects involving biomarker identification, knowledge discovery, predictive analytics for patient outcomes, and/or clinical application. R&D for advanced algorithms.  Core responsibilities include preprocessing of raw disparate biomedical datasets, development of automated knowledge extraction and feature engineering pipelines, and design of pilot studies/demos. The position entails working in a multi-disciplinary machine learning and deep learning team in close collaboration with bioinformatics experts, biologists, clinicians, as well as other leading academic and industry partners on impactful projects that have the potential to transform patient-care and deliver improved health outcomes.  ","About the Institute for Infocomm Research (I²R)  The Institute for Infocomm Research (I²R) is a member of the Agency for Science, Technology and Research (A*STAR) family and is Singapore’s largest ICT research institute. Our strategic thrusts are in the spheres of intelligence, communications and media and our research capabilities are in shared sensor networks, public-public/public-private data-sharing platform, big data analytics and visualisation solutions. For more information about I²R, please visit www.i2r.a-star.edu.sg The project focuses on development of machine learning, deep learning and artificial intelligence algorithms for applicants in precision medicine. Candidates should have demonstrated interests or experience in one or more of the following:  Analysis of large scale heterogeneous biomedical datastreams (genomics, EMR, imaging, lifestyle) Projects involving biomarker identification, knowledge discovery, predictive analytics for patient outcomes, and/or clinical application. R&D for advanced algorithms.  Core responsibilities include preprocessing of raw disparate biomedical datasets, development of automated knowledge extraction and feature engineering pipelines, and design of pilot studies/demos. The position entails working in a multi-disciplinary machine learning and deep learning team in close collaboration with bioinformatics experts, biologists, clinicians, as well as other leading academic and industry partners on impactful projects that have the potential to transform patient-care and deliver improved health outcomes.  "
Data Engineer,NTUC ENTERPRISE NEXUS CO-OPERATIVE LIMITED,Full Time,Professional,"Engineering, Information Technology",5000.0,10000.0,Monthly,"NTUC Enterprise Co-operative Limited is the holding entity and single largest shareholder of the NTUC group of Social Enterprises. We aim to create a greater social force to do good by harnessing the capabilities of the social enterprises to meet pressing social needs in areas like health and eldercare, childcare, daily essentials, cooked food and financial services. Serving over two million customers, NTUC Enterprise wants to enable and empower all in Singapore to live better and more meaningful lives. The NTUC Enterprise Centre of Excellence for Data, Digitalisation and Technology leads the transformation of the NTUC Social Enterprises by leveraging digital technologies to become more nimble, adaptable and innovative in today’s digital age. he NTUC Enterprise Centre of Excellence for Data, Digitalisation and Technology has been registered as NTUC Enterprise Nexus, a wholly owned subsidiary of NTUC Enterprise. The rapid adoption of technology and mobile devices have contributed to vast new flows of information which are larger in volume, faster in velocity, diverse in variety, and requires veracity of the information for use. This new type of information composed of structured and unstructured data, broadly known as big data (and combined with tools and platforms), if utilized well, could radically improve business performance.  As the organization embarks to become a data-driven organization, significant decisions and value generation will be based on the data that we capture and deploy. The 7 SE’s range in a broad scope of data from FairPrice (retail), Income (insurance), Unity (healthcare), FoodFare (F&B), LearningHub (training), First Campus(ECE), Link (membership). The leaders of these groups are keen to utilize the data to drive growth, deliver customer service, and create personalized experiences. The Data Architecture & Information Management Team will manage and govern the overall datasets of the organization and drive the execution of how the data will be collected, stored, processed and applied across these social enterprises (SEs). In this role you will work with various industries and most diverse datasets in Singapore.    Responsibilities: ·       As a data engineer, you will be creating, writing and maintaining data transfer process and protocols for the data platform.  ·       Work closely with each SE tech heads and their external vendors in mapping out data fields and data transfer process.  ·       Design, build, support and optimize new and existing data models and ETL processes. ·       Develop and support the data pipeline to integrate new data from various data sources with emerging data technologies. ·       Develop and manage the various dashboards for management decision and data visualizations. ·       Define and manage SLA for all data processes and own data quality issues.  ","NTUC Enterprise Co-operative Limited is the holding entity and single largest shareholder of the NTUC group of Social Enterprises. We aim to create a greater social force to do good by harnessing the capabilities of the social enterprises to meet pressing social needs in areas like health and eldercare, childcare, daily essentials, cooked food and financial services. Serving over two million customers, NTUC Enterprise wants to enable and empower all in Singapore to live better and more meaningful lives. The NTUC Enterprise Centre of Excellence for Data, Digitalisation and Technology leads the transformation of the NTUC Social Enterprises by leveraging digital technologies to become more nimble, adaptable and innovative in today’s digital age. he NTUC Enterprise Centre of Excellence for Data, Digitalisation and Technology has been registered as NTUC Enterprise Nexus, a wholly owned subsidiary of NTUC Enterprise. The rapid adoption of technology and mobile devices have contributed to vast new flows of information which are larger in volume, faster in velocity, diverse in variety, and requires veracity of the information for use. This new type of information composed of structured and unstructured data, broadly known as big data (and combined with tools and platforms), if utilized well, could radically improve business performance.  As the organization embarks to become a data-driven organization, significant decisions and value generation will be based on the data that we capture and deploy. The 7 SE’s range in a broad scope of data from FairPrice (retail), Income (insurance), Unity (healthcare), FoodFare (F&B), LearningHub (training), First Campus(ECE), Link (membership). The leaders of these groups are keen to utilize the data to drive growth, deliver customer service, and create personalized experiences. The Data Architecture & Information Management Team will manage and govern the overall datasets of the organization and drive the execution of how the data will be collected, stored, processed and applied across these social enterprises (SEs). In this role you will work with various industries and most diverse datasets in Singapore.    Responsibilities: ·       As a data engineer, you will be creating, writing and maintaining data transfer process and protocols for the data platform.  ·       Work closely with each SE tech heads and their external vendors in mapping out data fields and data transfer process.  ·       Design, build, support and optimize new and existing data models and ETL processes. ·       Develop and support the data pipeline to integrate new data from various data sources with emerging data technologies. ·       Develop and manage the various dashboards for management decision and data visualizations. ·       Define and manage SLA for all data processes and own data quality issues.  "
Data Engineer,NTUC LINK PRIVATE LIMITED,Full Time,Professional,"Engineering, Information Technology",5000.0,10000.0,Monthly,"The rapid adoption of technology and mobile devices have contributed to vast new flows of information which are larger in volume, faster in velocity, diverse in variety, and requires veracity of the information for use. This new type of information composed of structured and unstructured data, broadly known as big data (and combined with tools and platforms), if utilized well, could radically improve business performance.    As the organization embarks to become a data-driven organization, significant decisions and value generation will be based on the data that we capture and deploy. The 7 SE’s range in a broad scope of data from FairPrice (retail), Income (insurance), Unity (healthcare), FoodFare (F&B), LearningHub (training), First Campus(ECE), Link (membership). The leaders of these groups are keen to utilize the data to drive growth, deliver customer service, and create personalized experiences. The Data Architecture & Information Management Team will manage and govern the overall datasets of the organization and drive the execution of how the data will be collected, stored, processed and applied across these social enterprises (SEs). In this role you will work with various industries and most diverse datasets in Singapore.    Responsibility: ·       As a data engineer, you will be creating, writing and maintaining data transfer process and protocols for the data platform.  ·       Work closely with each SE tech heads and their external vendors in mapping out data fields and data transfer process.  ·       Design, build, support and optimize new and existing data models and ETL processes. ·       Develop and support the data pipeline to integrate new data from various data sources with emerging data technologies. ·       Develop and manage the various dashboards for management decision and data visualizations. ·       Define and manage SLA for all data processes and own data quality issues.  ","The rapid adoption of technology and mobile devices have contributed to vast new flows of information which are larger in volume, faster in velocity, diverse in variety, and requires veracity of the information for use. This new type of information composed of structured and unstructured data, broadly known as big data (and combined with tools and platforms), if utilized well, could radically improve business performance.    As the organization embarks to become a data-driven organization, significant decisions and value generation will be based on the data that we capture and deploy. The 7 SE’s range in a broad scope of data from FairPrice (retail), Income (insurance), Unity (healthcare), FoodFare (F&B), LearningHub (training), First Campus(ECE), Link (membership). The leaders of these groups are keen to utilize the data to drive growth, deliver customer service, and create personalized experiences. The Data Architecture & Information Management Team will manage and govern the overall datasets of the organization and drive the execution of how the data will be collected, stored, processed and applied across these social enterprises (SEs). In this role you will work with various industries and most diverse datasets in Singapore.    Responsibility: ·       As a data engineer, you will be creating, writing and maintaining data transfer process and protocols for the data platform.  ·       Work closely with each SE tech heads and their external vendors in mapping out data fields and data transfer process.  ·       Design, build, support and optimize new and existing data models and ETL processes. ·       Develop and support the data pipeline to integrate new data from various data sources with emerging data technologies. ·       Develop and manage the various dashboards for management decision and data visualizations. ·       Define and manage SLA for all data processes and own data quality issues.  "
Lead Data Engineer,JEWEL PAYMENTECH PTE. LTD.,"Permanent, Full Time",Professional,Information Technology,8000.0,10000.0,Monthly,"As a Data Engineer you will get to work on a wide range of problems using the cutting-edge technologies in Big Data and Data Science. You are required to translate Data Science and Machine learning based solutions into scalable code, and to develop innovative solutions to collect/cleanse/store/process data. In case you are very passionate about building high throughput, low latency, fault tolerant software then this position is for you.  To be successful in this role, you will need to:  Capture/Analyse requirements and lead the design/architecture of solutions to meet requirements.  Write code by using best software development practices/security standards. Lead projects end-to-end from conceptualisation to deployment. Write clear & concise documentation for solutions/code. Contribute ideas within team to build better code.  Continuously improve knowledge on new technologies. Excellent in English, both written and spoken.","As a Data Engineer you will get to work on a wide range of problems using the cutting-edge technologies in Big Data and Data Science. You are required to translate Data Science and Machine learning based solutions into scalable code, and to develop innovative solutions to collect/cleanse/store/process data. In case you are very passionate about building high throughput, low latency, fault tolerant software then this position is for you.  To be successful in this role, you will need to:  Capture/Analyse requirements and lead the design/architecture of solutions to meet requirements.  Write code by using best software development practices/security standards. Lead projects end-to-end from conceptualisation to deployment. Write clear & concise documentation for solutions/code. Contribute ideas within team to build better code.  Continuously improve knowledge on new technologies. Excellent in English, both written and spoken."
Data Engineer,HOOQ DIGITAL PTE. LTD.,Permanent,Manager,"Engineering, Information Technology",,,,"We are looking for a Data Engineer to join our rapidly expanding Data & Analytics team. You will help shape how we build and grow our service in the region. We look for self-starters who demand the best. Key Responsibilities:   Develop ETL/ELT jobs to integrate new data into the data warehouse or build new reporting schemas   Develop data pipelines, both bath and realtime from various platforms into the data lake   Manage various data platforms and seek out new technologies to improve efficiency   Develop advanced analytical models that help the business identify trends within customer base and behavior   Work closely with the business to understand data needs and create data sets to enable reporting and dashboards to monitor business performance   Ensure the data warehouse load jobs run as per schedule and data availability to the business is uninterrupted   Continuously improve the information management platforms of the company to leverage benefits ","We are looking for a Data Engineer to join our rapidly expanding Data & Analytics team. You will help shape how we build and grow our service in the region. We look for self-starters who demand the best. Key Responsibilities:   Develop ETL/ELT jobs to integrate new data into the data warehouse or build new reporting schemas   Develop data pipelines, both bath and realtime from various platforms into the data lake   Manage various data platforms and seek out new technologies to improve efficiency   Develop advanced analytical models that help the business identify trends within customer base and behavior   Work closely with the business to understand data needs and create data sets to enable reporting and dashboards to monitor business performance   Ensure the data warehouse load jobs run as per schedule and data availability to the business is uninterrupted   Continuously improve the information management platforms of the company to leverage benefits "
Data Engineer,SENSORFLOW PTE. LTD.,Permanent,Senior Executive,Information Technology,6000.0,8000.0,Monthly,"At SensorFlow we are planning for considerable growth over the next 12 months and need a data engineer to design and develop SensorFlow’s data infrastructure. As we are looking to build the data pipeline from scratch, you will have full autonomy and the technical backing from our engineering team in designing, developing and maintaining this infrastructure. Job Roles & Responsibilities   Design, develop and maintain SensorFlow’s infrastructure for streaming, processing and storage of data. Build tools for effective maintenance and monitoring of the data infrastructure.   Contribute to key data pipeline architecture decisions and lead the implementation of major initiatives.   Work closely with stakeholders to develop scalable and performant solutions for their data requirements, including extraction, transformation and loading of data from a range of data sources.   Develop the team’s data capabilities – share knowledge, enforce best practices and encourage data-driven decisions.  ","At SensorFlow we are planning for considerable growth over the next 12 months and need a data engineer to design and develop SensorFlow’s data infrastructure. As we are looking to build the data pipeline from scratch, you will have full autonomy and the technical backing from our engineering team in designing, developing and maintaining this infrastructure. Job Roles & Responsibilities   Design, develop and maintain SensorFlow’s infrastructure for streaming, processing and storage of data. Build tools for effective maintenance and monitoring of the data infrastructure.   Contribute to key data pipeline architecture decisions and lead the implementation of major initiatives.   Work closely with stakeholders to develop scalable and performant solutions for their data requirements, including extraction, transformation and loading of data from a range of data sources.   Develop the team’s data capabilities – share knowledge, enforce best practices and encourage data-driven decisions.  "
Data Engineer,GUMI ASIA PTE. LTD.,Full Time,"Professional, Senior Executive",Information Technology,5000.0,7000.0,Monthly," Design, develop, implement, and evolve data pipelines powering core data sets and key business and performance metrics Identify, troubleshoot, and resolve any performance, system or data related issues, and work to ensure data consistency and integrity Work with Product and Marketing teams on data requirements.  Work with various game teams on data set and data flow to ensure that data requirements are met. Ensure the quality, accuracy, and timeliness of analytical data "," Design, develop, implement, and evolve data pipelines powering core data sets and key business and performance metrics Identify, troubleshoot, and resolve any performance, system or data related issues, and work to ensure data consistency and integrity Work with Product and Marketing teams on data requirements.  Work with various game teams on data set and data flow to ensure that data requirements are met. Ensure the quality, accuracy, and timeliness of analytical data "
Senior Level Data Engineer,TRAVELOKA SERVICES PTE. LTD.,Permanent,Senior Executive,Information Technology,8300.0,15000.0,Monthly,"Data Engineers at Traveloka are passionate on designing, building, and maintaining our growing big data platform. We collect millions of events everyday to our data lake for insights, and to our real time pipeline for enabling many data-driven features such as personalized user experience. What you do will be mixed of software engineering, system architecture design, and operation:  You will be designing, building, supporting and scaling our data infrastructure. Including monitoring, alerting and debugging infrastructure that is streaming millions of events per hour with thousands of pipelines running Collaborate and coordinate with other departments (product, etc) to solve their use case using data technology; state of the art big data stack such as Kafka, Pubsub, Spark, DataFlow, BigQuery, Airflow, etc. on hundreds of terabytes data. Explore/learn new technologies that can complement or replace our current stack to improve it.   ","Data Engineers at Traveloka are passionate on designing, building, and maintaining our growing big data platform. We collect millions of events everyday to our data lake for insights, and to our real time pipeline for enabling many data-driven features such as personalized user experience. What you do will be mixed of software engineering, system architecture design, and operation:  You will be designing, building, supporting and scaling our data infrastructure. Including monitoring, alerting and debugging infrastructure that is streaming millions of events per hour with thousands of pipelines running Collaborate and coordinate with other departments (product, etc) to solve their use case using data technology; state of the art big data stack such as Kafka, Pubsub, Spark, DataFlow, BigQuery, Airflow, etc. on hundreds of terabytes data. Explore/learn new technologies that can complement or replace our current stack to improve it.   "
Mid -  Senior Level Data Engineer,TRAVELOKA SERVICES PTE. LTD.,Permanent,Executive,Information Technology,6100.0,10700.0,Monthly,"Data Engineers at Traveloka are passionate on designing, building, and maintaining our growing big data platform. We collect millions of events everyday to our data lake for insights, and to our real time pipeline for enabling many data-driven features such as personalized user experience. What you do will be mixed of software engineering, system architecture design, and operation:  You will be designing, building, supporting and scaling our data infrastructure. Including monitoring, alerting and debugging infrastructure that is streaming millions of events per hour with thousands of pipelines running Collaborate and coordinate with other departments (product, etc) to solve their use case using data technology; state of the art big data stack such as Kafka, Pubsub, Spark, DataFlow, BigQuery, Airflow, etc. on hundreds of terabytes data. Explore/learn new technologies that can complement or replace our current stack to improve it.   ","Data Engineers at Traveloka are passionate on designing, building, and maintaining our growing big data platform. We collect millions of events everyday to our data lake for insights, and to our real time pipeline for enabling many data-driven features such as personalized user experience. What you do will be mixed of software engineering, system architecture design, and operation:  You will be designing, building, supporting and scaling our data infrastructure. Including monitoring, alerting and debugging infrastructure that is streaming millions of events per hour with thousands of pipelines running Collaborate and coordinate with other departments (product, etc) to solve their use case using data technology; state of the art big data stack such as Kafka, Pubsub, Spark, DataFlow, BigQuery, Airflow, etc. on hundreds of terabytes data. Explore/learn new technologies that can complement or replace our current stack to improve it.   "
Mid Level Data Engineer,TRAVELOKA SERVICES PTE. LTD.,Permanent,Executive,Information Technology,4300.0,7600.0,Monthly,"Data Engineers at Traveloka are passionate on designing, building, and maintaining our growing big data platform. We collect millions of events everyday to our data lake for insights, and to our real time pipeline for enabling many data-driven features such as personalized user experience. What you do will be mixed of software engineering, system architecture design, and operation:  You will be designing, building, supporting and scaling our data infrastructure. Including monitoring, alerting and debugging infrastructure that is streaming millions of events per hour with thousands of pipelines running Collaborate and coordinate with other departments (product, etc) to solve their use case using data technology; state of the art big data stack such as Kafka, Pubsub, Spark, DataFlow, BigQuery, Airflow, etc. on hundreds of terabytes data. Explore/learn new technologies that can complement or replace our current stack to improve it. 	  ","Data Engineers at Traveloka are passionate on designing, building, and maintaining our growing big data platform. We collect millions of events everyday to our data lake for insights, and to our real time pipeline for enabling many data-driven features such as personalized user experience. What you do will be mixed of software engineering, system architecture design, and operation:  You will be designing, building, supporting and scaling our data infrastructure. Including monitoring, alerting and debugging infrastructure that is streaming millions of events per hour with thousands of pipelines running Collaborate and coordinate with other departments (product, etc) to solve their use case using data technology; state of the art big data stack such as Kafka, Pubsub, Spark, DataFlow, BigQuery, Airflow, etc. on hundreds of terabytes data. Explore/learn new technologies that can complement or replace our current stack to improve it. 	  "
Junior - Mid Level Data Engineer,TRAVELOKA SERVICES PTE. LTD.,Permanent,Fresh/entry level,Information Technology,3000.0,6000.0,Monthly,"Data Engineers at Traveloka are passionate on designing, building, and maintaining our growing big data platform. We collect millions of events everyday to our data lake for insights, and to our real time pipeline for enabling many data-driven features such as personalized user experience. What you do will be mixed of software engineering, system architecture design, and operation:  You will be designing, building, supporting and scaling our data infrastructure. Including monitoring, alerting and debugging infrastructure that is streaming millions of events per hour with thousands of pipelines running Collaborate and coordinate with other departments (product, etc) to solve their use case using data technology; state of the art big data stack such as Kafka, Pubsub, Spark, DataFlow, BigQuery, Airflow, etc. on hundreds of terabytes data. Explore/learn new technologies that can complement or replace our current stack to improve it. 	  ","Data Engineers at Traveloka are passionate on designing, building, and maintaining our growing big data platform. We collect millions of events everyday to our data lake for insights, and to our real time pipeline for enabling many data-driven features such as personalized user experience. What you do will be mixed of software engineering, system architecture design, and operation:  You will be designing, building, supporting and scaling our data infrastructure. Including monitoring, alerting and debugging infrastructure that is streaming millions of events per hour with thousands of pipelines running Collaborate and coordinate with other departments (product, etc) to solve their use case using data technology; state of the art big data stack such as Kafka, Pubsub, Spark, DataFlow, BigQuery, Airflow, etc. on hundreds of terabytes data. Explore/learn new technologies that can complement or replace our current stack to improve it. 	  "
Data Engineer,JEWEL PAYMENTECH PTE. LTD.,"Permanent, Full Time",Professional,Information Technology,4000.0,5000.0,Monthly,"As a Data Engineer you will get to work on a wide range of problems using the cutting-edge technologies in Big Data and Data Science. You are required to translate Data Science and Machine learning based solutions into scalable code, and to develop innovative solutions to collect/cleanse/store/process data. In case you are very passionate about building high throughput, low latency, fault tolerant software then this position is for you. To be successful in this role, you will need to:  Analyze requirements and deliver solutions that meet requirements. Write code by using best software development practices. Produce code that meets security standards. Estimate timelines and deliver solutions within agreed timeline. Write clear & concise documentation for solutions/code. Contribute ideas within team to build better code. Continuously improve knowledge on new technologies. Excellent in English, both written and spoken. ","As a Data Engineer you will get to work on a wide range of problems using the cutting-edge technologies in Big Data and Data Science. You are required to translate Data Science and Machine learning based solutions into scalable code, and to develop innovative solutions to collect/cleanse/store/process data. In case you are very passionate about building high throughput, low latency, fault tolerant software then this position is for you. To be successful in this role, you will need to:  Analyze requirements and deliver solutions that meet requirements. Write code by using best software development practices. Produce code that meets security standards. Estimate timelines and deliver solutions within agreed timeline. Write clear & concise documentation for solutions/code. Contribute ideas within team to build better code. Continuously improve knowledge on new technologies. Excellent in English, both written and spoken. "
L1 Desktop Engineer - Scotts Road - OS Backup & Server’s Data Backup (A1),THE SUPREME HR ADVISORY PTE. LTD.,Full Time,Senior Executive,Information Technology,1800.0,2100.0,Monthly,location: Scotts Road   ,location: Scotts Road   
"Data Engineer, Modeling & Onboarding",EYEOTA PTE. LTD.,Full Time,Middle Management,Information Technology,8000.0,15000.0,Monthly,"Eyeota is looking for an exceptional Data Engineer who can contribute to building a world-class big data engineering stack that will be used to fuel our Machine Learning product pipeline. This person will be contributing to the architecture, operation, and enhancement of:   Our petabyte-scale data platform with a key focus on finding solutions that can support the Machine Learning product roadmap. This platform ingests terabytes of data daily which need to be made available to a variety of Machine Learning use cases. Our bespoke Machine Learning pipelines. This will also provide opportunities to contribute to the prototyping, building, and deployment of Machine Learning models.  The candidate should have significant experience in developing and operating a modern data pipeline platform and should have a keen interest in Machine Learning and Data Science","Eyeota is looking for an exceptional Data Engineer who can contribute to building a world-class big data engineering stack that will be used to fuel our Machine Learning product pipeline. This person will be contributing to the architecture, operation, and enhancement of:   Our petabyte-scale data platform with a key focus on finding solutions that can support the Machine Learning product roadmap. This platform ingests terabytes of data daily which need to be made available to a variety of Machine Learning use cases. Our bespoke Machine Learning pipelines. This will also provide opportunities to contribute to the prototyping, building, and deployment of Machine Learning models.  The candidate should have significant experience in developing and operating a modern data pipeline platform and should have a keen interest in Machine Learning and Data Science"
Data Engineer (Python / Central),PEOPLE PROFILERS PTE. LTD.,Permanent,Junior Executive,Information Technology,4000.0,6000.0,Monthly," Join a fast-expanding and highly-specialised tech company Good exposure in data analytics projects Central location Great company culture & working environment    Requirements  Make enhancements to data collection, structure & delivery Use automated methods for data compilation Build robust batch & streaming pipelines Write clear and high-quality code (including in Python) Work well in a team "," Join a fast-expanding and highly-specialised tech company Good exposure in data analytics projects Central location Great company culture & working environment    Requirements  Make enhancements to data collection, structure & delivery Use automated methods for data compilation Build robust batch & streaming pipelines Write clear and high-quality code (including in Python) Work well in a team "
Senior Engineer  /  Engineer - Data Management,INFINEON TECHNOLOGIES ASIA PACIFIC PTE LTD,"Permanent, Full Time",Executive,"Engineering, Information Technology, Manufacturing",3000.0,6000.0,Monthly,"You are responsible to support the automated / productivity projects which are relevant to Automated Test Equipment (ATE), and improve data handling towards automation. In your new role you will:  Support automated / productivity projects relevant with Automated Test Equipment (ATE) & data handling Improve data clarity with scripting towards yield improvement Drive & improve data relevant KPI (data quality, process time) towards data automation Responsible for production of automated computer hardware and software for test equipment Install and configure, investigate, diagnose and solve ATE computer software and hardware issues ","You are responsible to support the automated / productivity projects which are relevant to Automated Test Equipment (ATE), and improve data handling towards automation. In your new role you will:  Support automated / productivity projects relevant with Automated Test Equipment (ATE) & data handling Improve data clarity with scripting towards yield improvement Drive & improve data relevant KPI (data quality, process time) towards data automation Responsible for production of automated computer hardware and software for test equipment Install and configure, investigate, diagnose and solve ATE computer software and hardware issues "
Lead Electrical Engineer (Data Centre),PM ASIA PROJECT SERVICES PTE. LTD.,"Permanent, Contract",Senior Executive,Engineering,7500.0,11000.0,Monthly,"Overview:  Lead the Electrical Design of large scale, high end industrial facilities.  Have knowledge and experience in designing Electrical systems for Data Centres, including Medium Voltage, Low Voltage Power Systems, Emergency Power Systems, ELV’s (CCTV, Interlocks, Fire Alarm, Power Monitoring System) Work in a multi-disciplinary design office environment for global clients. Display a personal commitment to safety, hold safety as a core value and provide safety leadership in the performance of all work activities Be quality focussed, producing well engineered designs to the highest standards in an ISO9000 Quality System environment   ","Overview:  Lead the Electrical Design of large scale, high end industrial facilities.  Have knowledge and experience in designing Electrical systems for Data Centres, including Medium Voltage, Low Voltage Power Systems, Emergency Power Systems, ELV’s (CCTV, Interlocks, Fire Alarm, Power Monitoring System) Work in a multi-disciplinary design office environment for global clients. Display a personal commitment to safety, hold safety as a core value and provide safety leadership in the performance of all work activities Be quality focussed, producing well engineered designs to the highest standards in an ISO9000 Quality System environment   "
Big Data Engineer (287102),SIEMENS PTE. LTD.,"Permanent, Full Time","Professional, Senior Executive",Information Technology,4000.0,7000.0,Monthly,"What are my responsibilities?  Responsible for the integration of large, structured and unstructured data volumes into the existing cloud platforms Development of scalable end-to-end data pipelines for batch and stream processing Execution of the data integration activities (ETL /ELT) for populating the data lake and integrating diverse data sources Execution an further development of the physical implementation of the logical data model into a physical implementation in the data lake Implementation of solutions for reference data and master data management within the context of the mobility data business Execution of data quality measurements and implementation of data quality improvement ","What are my responsibilities?  Responsible for the integration of large, structured and unstructured data volumes into the existing cloud platforms Development of scalable end-to-end data pipelines for batch and stream processing Execution of the data integration activities (ETL /ELT) for populating the data lake and integrating diverse data sources Execution an further development of the physical implementation of the logical data model into a physical implementation in the data lake Implementation of solutions for reference data and master data management within the context of the mobility data business Execution of data quality measurements and implementation of data quality improvement "
High-Performance Data Engineer,NIOMETRICS (PTE.) LTD.,Permanent,Professional,Information Technology,5500.0,11000.0,Monthly,"WHAT WE DO We invite you to be part of our ambitious, close-knit team creating systems for large customers who need to crunch through Tbps of data in real-time. Our approach is relentless performance-oriented software engineering vs. server sprawl in our customers' datacentres. You will use the latest high-end hardware and continuously devise ways to push the envelope of software performance. We build in-house systems if we must. We had to for indexing 1M 60-column rows/s, for aggregating high throughput event streams over hundreds of combinations of dimensions, and for pattern matching 5M patterns at 100Gb/s per 2RU. We use these to solve real customer problems. You will experiment wildly. For example we implemented network monitoring using a GPU, and we tested 4-socket machines with 2T RAM. Our current favourite platform is a 2-socket system with E5-2699v4 CPUs (88 lcores in total), 4x40Gbps NICs and 1T RAM, which we use to process 160Gbps. You will help us build a successful software platform for the long run. We invest a lot in flexibility, such as with our extensible rule engine and declarative aggregation system that empowers our analysts and helps us minimise the C code we have to write for supporting disparate use-cases. We know the devil is in the details. You will improve performance through better memory allocation systems and better data structures, all while ensuring that they are integrated with Address Sanitizer and fully tested using unit tests and end-to-end regression tests. We work end-to-end. You will implement data engineering solutions that are both efficient and secure for handling events from 500 million users, and to extract insight without leaking individual information. We want to show off. To attract the best programmers we plan to showcase our technology. You can be part of our effort to open-source interesting pieces of our technology stack.   YOUR ROLE AS HIGH-PERFORMANCE DATA ENGINEER As a High-Performance Data Engineer, you will create and maintain tools, mainly in C, for crunching large amounts of data in files or streams. You will have to think both big, in terms of overall architecture, and small, in terms of low-level optimisations, to deliver solutions that are reusable, and match the performance of the best hardware. Every capability you add directly translates to new offerings made possible. Every percent of performance improvement directly translates to large cost savings. At the same time, the correctness and reliability of your work will be the cornerstone to our customers’ trust.","WHAT WE DO We invite you to be part of our ambitious, close-knit team creating systems for large customers who need to crunch through Tbps of data in real-time. Our approach is relentless performance-oriented software engineering vs. server sprawl in our customers' datacentres. You will use the latest high-end hardware and continuously devise ways to push the envelope of software performance. We build in-house systems if we must. We had to for indexing 1M 60-column rows/s, for aggregating high throughput event streams over hundreds of combinations of dimensions, and for pattern matching 5M patterns at 100Gb/s per 2RU. We use these to solve real customer problems. You will experiment wildly. For example we implemented network monitoring using a GPU, and we tested 4-socket machines with 2T RAM. Our current favourite platform is a 2-socket system with E5-2699v4 CPUs (88 lcores in total), 4x40Gbps NICs and 1T RAM, which we use to process 160Gbps. You will help us build a successful software platform for the long run. We invest a lot in flexibility, such as with our extensible rule engine and declarative aggregation system that empowers our analysts and helps us minimise the C code we have to write for supporting disparate use-cases. We know the devil is in the details. You will improve performance through better memory allocation systems and better data structures, all while ensuring that they are integrated with Address Sanitizer and fully tested using unit tests and end-to-end regression tests. We work end-to-end. You will implement data engineering solutions that are both efficient and secure for handling events from 500 million users, and to extract insight without leaking individual information. We want to show off. To attract the best programmers we plan to showcase our technology. You can be part of our effort to open-source interesting pieces of our technology stack.   YOUR ROLE AS HIGH-PERFORMANCE DATA ENGINEER As a High-Performance Data Engineer, you will create and maintain tools, mainly in C, for crunching large amounts of data in files or streams. You will have to think both big, in terms of overall architecture, and small, in terms of low-level optimisations, to deliver solutions that are reusable, and match the performance of the best hardware. Every capability you add directly translates to new offerings made possible. Every percent of performance improvement directly translates to large cost savings. At the same time, the correctness and reliability of your work will be the cornerstone to our customers’ trust."
Data Center (System) Engineer,CAPITA PTE. LTD.,"Contract, Full Time",Junior Executive,Information Technology,2500.0,3000.0,Monthly,"​Responsibility   Monitoring of ESX, OS Batch Monitoring Service Request Execution Coordinate with Appointed vendor for Offsite tape archival Restoration of VM/s Batch execution and monitoring, batch output & event handling, batch incident identification, escalation & reporting Tape backups, media handling, tape library operations and off-site storage Server health checks Start-up, shutdown, reboot/restart systems & services Generate daily/ weekly/ monthly/ yearly/ ad-hoc reports and dispatch reports to users & customers Adhere to all operational & physical security procedures Provide operational support during DR Exercises at DR sites Work towards acceptable audit rating ","​Responsibility   Monitoring of ESX, OS Batch Monitoring Service Request Execution Coordinate with Appointed vendor for Offsite tape archival Restoration of VM/s Batch execution and monitoring, batch output & event handling, batch incident identification, escalation & reporting Tape backups, media handling, tape library operations and off-site storage Server health checks Start-up, shutdown, reboot/restart systems & services Generate daily/ weekly/ monthly/ yearly/ ad-hoc reports and dispatch reports to users & customers Adhere to all operational & physical security procedures Provide operational support during DR Exercises at DR sites Work towards acceptable audit rating "
Data Development Engineer,Company Undisclosed,Full Time,Professional,Banking and Finance,6000.0,8000.0,Monthly,"WorldQuant develops and deploys systematic financial strategies across a variety of asset classes and global markets. We seek to produce high-quality predictive signals (Alphas) through our proprietary research platform to employ financial strategies focused on exploiting market inefficiencies. Our teams work collaboratively to drive the production of Alphas and financial strategies – the foundation of a sustainable, global investment platform.   Technologists at WorldQuant research, design, code, test and deploy projects while working collaboratively with researchers and portfolio managers. Our environment is relaxed yet intellectually intense. Our teams are lean and agile, which means rapid prototyping of products with immediate user feedback. We seek people who think in code, aspire to solve undiscovered computer science challenges and are motivated by being around like-minded people. In fact, of the 600 employees globally, approximately 500 of them code on a daily basis.   WorldQuant’s success is built on a culture that pairs academic sensibility with accountability for results. Employees are encouraged to think openly about problems, balancing intellectualism and practicality. Great ideas come from anyone, anywhere. Employees are encouraged to challenge conventional thinking and possess a mindset of continuous improvement. That’s a key ingredient in remaining a leader in any industry.      Our goal is to hire the best and the brightest. We value intellectual horsepower first and foremost, and people who demonstrate an exceptional talent. There is no roadmap to future success, so we need people who can help us create it. Our collective intelligence will drive us there.   The Role: In this role, candidate will implement and maintain software towards creation of new datasets. Data sets will be consumed internally by researchers and utilized by internal quantitative models. Candidate will design efficient algorithms for collection, analysis, processing and filtering of data.    Work with the global team in designing and implementing data retrieval software for various data sets Implement the rules and procedures that ensure integrity in data sets Collect and analyze statistics on market data applications and devise approaches to improve the relevant processes Develop and enhance monitoring tools to detect various types of errors in data ","WorldQuant develops and deploys systematic financial strategies across a variety of asset classes and global markets. We seek to produce high-quality predictive signals (Alphas) through our proprietary research platform to employ financial strategies focused on exploiting market inefficiencies. Our teams work collaboratively to drive the production of Alphas and financial strategies – the foundation of a sustainable, global investment platform.   Technologists at WorldQuant research, design, code, test and deploy projects while working collaboratively with researchers and portfolio managers. Our environment is relaxed yet intellectually intense. Our teams are lean and agile, which means rapid prototyping of products with immediate user feedback. We seek people who think in code, aspire to solve undiscovered computer science challenges and are motivated by being around like-minded people. In fact, of the 600 employees globally, approximately 500 of them code on a daily basis.   WorldQuant’s success is built on a culture that pairs academic sensibility with accountability for results. Employees are encouraged to think openly about problems, balancing intellectualism and practicality. Great ideas come from anyone, anywhere. Employees are encouraged to challenge conventional thinking and possess a mindset of continuous improvement. That’s a key ingredient in remaining a leader in any industry.      Our goal is to hire the best and the brightest. We value intellectual horsepower first and foremost, and people who demonstrate an exceptional talent. There is no roadmap to future success, so we need people who can help us create it. Our collective intelligence will drive us there.   The Role: In this role, candidate will implement and maintain software towards creation of new datasets. Data sets will be consumed internally by researchers and utilized by internal quantitative models. Candidate will design efficient algorithms for collection, analysis, processing and filtering of data.    Work with the global team in designing and implementing data retrieval software for various data sets Implement the rules and procedures that ensure integrity in data sets Collect and analyze statistics on market data applications and devise approaches to improve the relevant processes Develop and enhance monitoring tools to detect various types of errors in data "
MCT Big Data Engineer,Company Undisclosed,"Permanent, Full Time",Non-executive,"Engineering, Manufacturing",3400.0,6800.0,Monthly,"Do you have a broad theoretical and practical understanding of data engineering and data science? Can you wrangle large scale multidimensional data effectively? Are you always curious to learn something new? Do you love to solve engineering puzzles and optimize complex systems? Can you translate an idea in to an algorithm and make it into a product with quality and scalability in mind? Are you looking for window to the world ?   If so, you may be a great candidate for an Manufacturing Central Team (MCT) Data Engineer position at company, a global, Fortune 500 leader in the semiconductor industry. This position will be based in Singapore.   As an MCT Data Engineer at company, you will:   * Work with an international team of data scientists, data engineers, software engineers, process and equipment engineers, process integration engineers, yield enhancement engineers, R&D, etc. in a collaborative manner to develop new data science solutions that improve quality, improve yield, reduce deviations, improve manufacturing cycle time, reduce cost, extend manufacturing capabilities, etc. * Draw from a broad background of data-mining techniques in mathematics, statistics, information technology, machine learning, data engineering, design of experiments (DOE), visualization, etc. to discover insightful patterns in semiconductor manufacturing data * Work on projects and develop solutions that would be of high impact to various areas at all manufacturing fabs * Deliver polished presentations of data acquisition, data flow, data preparation and data presentation layer to internal customers and leaders to inform business strategy, streamline operations, and execute to revenue goals * In short, be a full-stack data engineer who can take an idea, access and prepare necessary data, work with data scientists to create machine learning models, develop it to an application with intuitive user interface, integrate with any pre-existing systems, demonstrate successful use cases and wins, etc.   Responsibilities and Tasks include, but not limited to:   * Understanding business needs and strategy to develop data science solutions * Collaborating with other data engineers and business process experts to access existing data in data warehouse and big data environments * Developing new or enhancing prior data acquisition and ETL pipelines from various sources into big data ecosystem. * Preparing data for machine learning using appropriate steps and methods, which may include data cleaning, transformation, augmentation, enrichment, sampling, etc. * Working with various scientific data such as equipment sensor data and logs, image and various types of signals, manufacturing process data, etc. to extract meaningful information for analytics * Creating intuitive user interface for interactive data visualization to explain insights from data * Preparing and delivering powerful presentations with rich data visualizations and meaningful business conclusions * Documenting the train of thoughts used to design and implement solutions along with managed source code * Traveling and participating in various internal forums for strategy building and to build solutions in collaboration with various manufacturing sites","Do you have a broad theoretical and practical understanding of data engineering and data science? Can you wrangle large scale multidimensional data effectively? Are you always curious to learn something new? Do you love to solve engineering puzzles and optimize complex systems? Can you translate an idea in to an algorithm and make it into a product with quality and scalability in mind? Are you looking for window to the world ?   If so, you may be a great candidate for an Manufacturing Central Team (MCT) Data Engineer position at company, a global, Fortune 500 leader in the semiconductor industry. This position will be based in Singapore.   As an MCT Data Engineer at company, you will:   * Work with an international team of data scientists, data engineers, software engineers, process and equipment engineers, process integration engineers, yield enhancement engineers, R&D, etc. in a collaborative manner to develop new data science solutions that improve quality, improve yield, reduce deviations, improve manufacturing cycle time, reduce cost, extend manufacturing capabilities, etc. * Draw from a broad background of data-mining techniques in mathematics, statistics, information technology, machine learning, data engineering, design of experiments (DOE), visualization, etc. to discover insightful patterns in semiconductor manufacturing data * Work on projects and develop solutions that would be of high impact to various areas at all manufacturing fabs * Deliver polished presentations of data acquisition, data flow, data preparation and data presentation layer to internal customers and leaders to inform business strategy, streamline operations, and execute to revenue goals * In short, be a full-stack data engineer who can take an idea, access and prepare necessary data, work with data scientists to create machine learning models, develop it to an application with intuitive user interface, integrate with any pre-existing systems, demonstrate successful use cases and wins, etc.   Responsibilities and Tasks include, but not limited to:   * Understanding business needs and strategy to develop data science solutions * Collaborating with other data engineers and business process experts to access existing data in data warehouse and big data environments * Developing new or enhancing prior data acquisition and ETL pipelines from various sources into big data ecosystem. * Preparing data for machine learning using appropriate steps and methods, which may include data cleaning, transformation, augmentation, enrichment, sampling, etc. * Working with various scientific data such as equipment sensor data and logs, image and various types of signals, manufacturing process data, etc. to extract meaningful information for analytics * Creating intuitive user interface for interactive data visualization to explain insights from data * Preparing and delivering powerful presentations with rich data visualizations and meaningful business conclusions * Documenting the train of thoughts used to design and implement solutions along with managed source code * Traveling and participating in various internal forums for strategy building and to build solutions in collaboration with various manufacturing sites"
IT - BIG DATA ENGINEER,Company Undisclosed,"Permanent, Full Time",Non-executive,"Engineering, Manufacturing",3400.0,6800.0,Monthly,"Responsibilities: Be part of a DevOps team that design, build and maintain innovative Smart Manufacturing solutions and Big Data platform.  Participate in Agile development lifecycle for software & solution related to Smart Manufacturing and Big Data platform.   Work with Data Science within company to develop, automate and maintain reliable data analytic and mining solutions for Smart Manufacturing and Big Data platform.  Ability to assess current IT environments and make recommendations to increase capacity needs.  Communicate, collaborate and coordinate on Smart Manufacturing and Big Data related activities to various level of stakeholders and senior management.","Responsibilities: Be part of a DevOps team that design, build and maintain innovative Smart Manufacturing solutions and Big Data platform.  Participate in Agile development lifecycle for software & solution related to Smart Manufacturing and Big Data platform.   Work with Data Science within company to develop, automate and maintain reliable data analytic and mining solutions for Smart Manufacturing and Big Data platform.  Ability to assess current IT environments and make recommendations to increase capacity needs.  Communicate, collaborate and coordinate on Smart Manufacturing and Big Data related activities to various level of stakeholders and senior management."
MCT Big Data Senior Engineer,Company Undisclosed,"Permanent, Full Time",Non-executive,"Engineering, Manufacturing",5000.0,10000.0,Monthly,"Do you have a broad theoretical and practical understanding of data engineering and data science? Can you wrangle large scale multidimensional data effectively? Are you always curious to learn something new? Do you love to solve engineering puzzles and optimize complex systems? Can you translate an idea in to an algorithm and make it into a product with quality and scalability in mind? Are you looking for window to the world ?   If so, you may be a great candidate for an Manufacturing Central Team (MCT) Data Engineer position at company, a global, Fortune 500 leader in the semiconductor industry. This position will be based in Singapore.   As an MCT Data Engineer at company, you will:   * Work with an international team of data scientists, data engineers, software engineers, process and equipment engineers, process integration engineers, yield enhancement engineers, R&D, etc. in a collaborative manner to develop new data science solutions that improve quality, improve yield, reduce deviations, improve manufacturing cycle time, reduce cost, extend manufacturing capabilities, etc. * Draw from a broad background of data-mining techniques in mathematics, statistics, information technology, machine learning, data engineering, design of experiments (DOE), visualization, etc. to discover insightful patterns in semiconductor manufacturing data * Work on projects and develop solutions that would be of high impact to various areas at all manufacturing fabs * Deliver polished presentations of data acquisition, data flow, data preparation and data presentation layer to internal customers and leaders to inform business strategy, streamline operations, and execute to revenue goals * In short, be a full-stack data engineer who can take an idea, access and prepare necessary data, work with data scientists to create machine learning models, develop it to an application with intuitive user interface, integrate with any pre-existing systems, demonstrate successful use cases and wins, etc.   Responsibilities and Tasks include, but not limited to:   * Understanding business needs and strategy to develop data science solutions * Collaborating with other data engineers and business process experts to access existing data in data warehouse and big data environments * Developing new or enhancing prior data acquisition and ETL pipelines from various sources into big data ecosystem. * Preparing data for machine learning using appropriate steps and methods, which may include data cleaning, transformation, augmentation, enrichment, sampling, etc. * Working with various scientific data such as equipment sensor data and logs, image and various types of signals, manufacturing process data, etc. to extract meaningful information for analytics * Creating intuitive user interface for interactive data visualization to explain insights from data * Preparing and delivering powerful presentations with rich data visualizations and meaningful business conclusions * Documenting the train of thoughts used to design and implement solutions along with managed source code * Traveling and participating in various internal forums for strategy building and to build solutions in collaboration with various manufacturing sites","Do you have a broad theoretical and practical understanding of data engineering and data science? Can you wrangle large scale multidimensional data effectively? Are you always curious to learn something new? Do you love to solve engineering puzzles and optimize complex systems? Can you translate an idea in to an algorithm and make it into a product with quality and scalability in mind? Are you looking for window to the world ?   If so, you may be a great candidate for an Manufacturing Central Team (MCT) Data Engineer position at company, a global, Fortune 500 leader in the semiconductor industry. This position will be based in Singapore.   As an MCT Data Engineer at company, you will:   * Work with an international team of data scientists, data engineers, software engineers, process and equipment engineers, process integration engineers, yield enhancement engineers, R&D, etc. in a collaborative manner to develop new data science solutions that improve quality, improve yield, reduce deviations, improve manufacturing cycle time, reduce cost, extend manufacturing capabilities, etc. * Draw from a broad background of data-mining techniques in mathematics, statistics, information technology, machine learning, data engineering, design of experiments (DOE), visualization, etc. to discover insightful patterns in semiconductor manufacturing data * Work on projects and develop solutions that would be of high impact to various areas at all manufacturing fabs * Deliver polished presentations of data acquisition, data flow, data preparation and data presentation layer to internal customers and leaders to inform business strategy, streamline operations, and execute to revenue goals * In short, be a full-stack data engineer who can take an idea, access and prepare necessary data, work with data scientists to create machine learning models, develop it to an application with intuitive user interface, integrate with any pre-existing systems, demonstrate successful use cases and wins, etc.   Responsibilities and Tasks include, but not limited to:   * Understanding business needs and strategy to develop data science solutions * Collaborating with other data engineers and business process experts to access existing data in data warehouse and big data environments * Developing new or enhancing prior data acquisition and ETL pipelines from various sources into big data ecosystem. * Preparing data for machine learning using appropriate steps and methods, which may include data cleaning, transformation, augmentation, enrichment, sampling, etc. * Working with various scientific data such as equipment sensor data and logs, image and various types of signals, manufacturing process data, etc. to extract meaningful information for analytics * Creating intuitive user interface for interactive data visualization to explain insights from data * Preparing and delivering powerful presentations with rich data visualizations and meaningful business conclusions * Documenting the train of thoughts used to design and implement solutions along with managed source code * Traveling and participating in various internal forums for strategy building and to build solutions in collaboration with various manufacturing sites"
MCT QE Engineer (Data Specialist),Company Undisclosed,"Permanent, Full Time",Non-executive,"Engineering, Manufacturing",4000.0,8000.0,Monthly,"Responsibilities •As a Manufacturing Central Team Quality Engineer at company, you will be a member of the worldwide Quality Team responsible for QDRs, reducing variation and deviations for all company's Fabs. •Specific to your role as Data Specialist, your work will encompass the responsibilities of an architect, a designer and a developer/administrator. You need to have an in-depth understanding of database and structure and use those skills and knowledge to maintain their stability and reliability. •You will work with SMEs to identify their needs and to design and implement reporting dashboard and data structure to meet their requirements for quality improvement; your work will also extend to recommend improvements to meet the demands of swiftly changing interface technology; you will also be in charge of performing backups procedures to protect the data.","Responsibilities •As a Manufacturing Central Team Quality Engineer at company, you will be a member of the worldwide Quality Team responsible for QDRs, reducing variation and deviations for all company's Fabs. •Specific to your role as Data Specialist, your work will encompass the responsibilities of an architect, a designer and a developer/administrator. You need to have an in-depth understanding of database and structure and use those skills and knowledge to maintain their stability and reliability. •You will work with SMEs to identify their needs and to design and implement reporting dashboard and data structure to meet their requirements for quality improvement; your work will also extend to recommend improvements to meet the demands of swiftly changing interface technology; you will also be in charge of performing backups procedures to protect the data."
Senior Data Engineer,KPLER PTE. LTD.,Full Time,Senior Executive,Information Technology,7000.0,10000.0,Monthly,"Who are we ?   Kpler is an intelligence company providing transparency solutions in energy markets. We develop proprietary technologies that systematically aggregate data from hundreds of sources ranging from logistical and commercial, to governmental and shipping databases. By connecting the dots across fragmented information landscapes, we are able to provide our clients with unique, real-time market coverage.   We rely on intelligent people to build intelligent software. Our team is composed of individuals of various backgrounds, with diversified skill sets and international experiences. Our clients are players across the energy market spectrum, with offices from Houston to Singapore.   Role Purpose   We are looking for a Senior Data Engineer to join our software engineering team in Singapore to work on our data pipelines (collect, manage, data lake storage), data algorithms (based on either business rules, constraint programming, ML, etc.) and be a technical referent in Python and/or Scala. Our future team member will have a good understanding of data collection and management of complex B2B business rules. Also, you will take ownership of large features from technical design through completion. At Kpler, the Senior Data Engineer is at the centre of our research delivery and will have a key role in defining architecture, helping identify and implement areas for improvement within our data methodologies and technologies used. Based in Singapore, you will also interact with the Paris engineering team; being able to communicate efficiently in English (mandatory, we have more than 15 nationalities at Kpler!) and work with remote team members is key.   Also, you will:  Coach and work on code review with more junior engineers Ensure integrity of data through creative, robust and sustainable quality control methods Participate in operations/support of the real-time platforms Participate in defining coding standards, specifications and development processes Translating technical concepts to/from non-technical language ","Who are we ?   Kpler is an intelligence company providing transparency solutions in energy markets. We develop proprietary technologies that systematically aggregate data from hundreds of sources ranging from logistical and commercial, to governmental and shipping databases. By connecting the dots across fragmented information landscapes, we are able to provide our clients with unique, real-time market coverage.   We rely on intelligent people to build intelligent software. Our team is composed of individuals of various backgrounds, with diversified skill sets and international experiences. Our clients are players across the energy market spectrum, with offices from Houston to Singapore.   Role Purpose   We are looking for a Senior Data Engineer to join our software engineering team in Singapore to work on our data pipelines (collect, manage, data lake storage), data algorithms (based on either business rules, constraint programming, ML, etc.) and be a technical referent in Python and/or Scala. Our future team member will have a good understanding of data collection and management of complex B2B business rules. Also, you will take ownership of large features from technical design through completion. At Kpler, the Senior Data Engineer is at the centre of our research delivery and will have a key role in defining architecture, helping identify and implement areas for improvement within our data methodologies and technologies used. Based in Singapore, you will also interact with the Paris engineering team; being able to communicate efficiently in English (mandatory, we have more than 15 nationalities at Kpler!) and work with remote team members is key.   Also, you will:  Coach and work on code review with more junior engineers Ensure integrity of data through creative, robust and sustainable quality control methods Participate in operations/support of the real-time platforms Participate in defining coding standards, specifications and development processes Translating technical concepts to/from non-technical language "
Global Technology Infrastructure Data Center Operations Engineer I - Analyst,"JPMORGAN CHASE BANK, N.A.","Permanent, Full Time",Professional,Information Technology,4000.0,8000.0,Monthly,"As a Data Center Operations Engineer I, your mission is to support the day-to-day technology operations of JPMorgan Chase mission critical data centers. The purpose of this role is to maintain operational stability and handle customer requests while working on shifts and on calls to support the 24x7 operation. You will be responsible for installing and configuring enterprise class technology hardware, troubleshooting hardware and network issues, maintain change control process in the data center, and support 3rd party vendor activities. This position is full-time, working with team members on a rotating basis.  ","As a Data Center Operations Engineer I, your mission is to support the day-to-day technology operations of JPMorgan Chase mission critical data centers. The purpose of this role is to maintain operational stability and handle customer requests while working on shifts and on calls to support the 24x7 operation. You will be responsible for installing and configuring enterprise class technology hardware, troubleshooting hardware and network issues, maintain change control process in the data center, and support 3rd party vendor activities. This position is full-time, working with team members on a rotating basis.  "
Data Quality Engineer,ALLEGIS GROUP SINGAPORE PRIVATE LIMITED,Contract,Executive,Information Technology,9000.0,12000.0,Monthly,"This is a new initiative of the bank within a pioneer team to bring data quality into operations. The primary work comprises of ETL tool programming, data quality implementation, data governance and change & release management.  ","This is a new initiative of the bank within a pioneer team to bring data quality into operations. The primary work comprises of ETL tool programming, data quality implementation, data governance and change & release management.  "
Data Quality Engineer,ALLEGIS GROUP SINGAPORE PRIVATE LIMITED,Contract,Executive,Information Technology,5000.0,9000.0,Monthly,"This is a new initiative of the bank within a pioneer team to bring data quality into operations. The primary work comprises of ETL tool programming, data quality implementation, data governance and change & release management.  ","This is a new initiative of the bank within a pioneer team to bring data quality into operations. The primary work comprises of ETL tool programming, data quality implementation, data governance and change & release management.  "
"Senior Associate, DevOps Engineer, Group Consumer Banking and Big Data Analytics Tech (180004DY)",DBS BANK LTD.,"Permanent, Full Time",Senior Executive,Banking and Finance,5000.0,10000.0,Monthly,"Job Purpose  The role forms part of the Insurance and Investment Platform technology team. Develop and deliver technology solutions relating to web and mobile applications across wealth customer segments.   Key Accountabilities  Engineer CI/CT/CD pipeline that is optimized to run within minutes Enforce best practices in code quality and release/deployment process to achieve near zero production incidents    Responsibilities  Architect, build and maintain continuous integration, testing and deployment (CI/CT/CD) pipeline for web and mobile apps Collaborate with architects, development engineers and system administrators to provision and maintain the platform infrastructure both on premise as well as cloud (for development, test and production environments) Build and maintain system and application health check and house-keeping jobs Troubleshoot system and connectivity errors and follow up with administrators, vendors or other teams for timely resolution Develop, maintain and document best practices in source control management and infrastructure as code  Track, maintain and renew infrastructure, web and mobile application key-stores and profiles Track, enforce and maintain code quality, security and performance reports Identify improvement areas and engage the required stakeholders to successful implement the changes Keep track of evolving technologies and perform proof of concept integrations for successful platform integrations as per roadmap Maintain platform collaboration tools such as JIRA and Confluence ","Job Purpose  The role forms part of the Insurance and Investment Platform technology team. Develop and deliver technology solutions relating to web and mobile applications across wealth customer segments.   Key Accountabilities  Engineer CI/CT/CD pipeline that is optimized to run within minutes Enforce best practices in code quality and release/deployment process to achieve near zero production incidents    Responsibilities  Architect, build and maintain continuous integration, testing and deployment (CI/CT/CD) pipeline for web and mobile apps Collaborate with architects, development engineers and system administrators to provision and maintain the platform infrastructure both on premise as well as cloud (for development, test and production environments) Build and maintain system and application health check and house-keeping jobs Troubleshoot system and connectivity errors and follow up with administrators, vendors or other teams for timely resolution Develop, maintain and document best practices in source control management and infrastructure as code  Track, maintain and renew infrastructure, web and mobile application key-stores and profiles Track, enforce and maintain code quality, security and performance reports Identify improvement areas and engage the required stakeholders to successful implement the changes Keep track of evolving technologies and perform proof of concept integrations for successful platform integrations as per roadmap Maintain platform collaboration tools such as JIRA and Confluence "
Data Engineer,GOVERNMENT TECHNOLOGY AGENCY,Permanent,,"Information Technology, Public / Civil Service",,,,"The Government Digital Services team is seeking an accomplished Data Engineer. We are a team in GovTech that aims to design and develop software applications that help government agencies to better serve the needs of Singaporeans. We adopt an Agile development approach and work towards adopting tech best practices and cutting edge tools.
We are looking for a savvy Data Engineer to join our growing team of analytics experts. The hire will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up.
The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.
What To Expect:

Create and maintain optimal data pipeline architecture.
Assemble large, complex data sets that meet functional / non-functional business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and ‘big data’ technologies.
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with stakeholders including the Executive, Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs.
Keep our data separated and secure through multiple data centers.
Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.
Work with data and analytics experts to strive for greater functionality in our data systems.

How To Succeed:

Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Strong analytic skills related to working with unstructured datasets.
Build processes supporting data transformation, data structures, metadata, dependency and workload management.
A successful history of manipulating, processing and extracting value from large disconnected datasets.
Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
Experience supporting and working with cross-functional teams in a dynamic environment.
We are looking for a candidate with 5+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. Preferably with the experience of using the following software/tools:

Experience with big data tools: Hadoop, Spark, Kafka, RabbitMQ etc.
Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift
Experience with stream-processing systems: Storm, Spark-Streaming, etc.
Experience with either of these languages: Python, Java.


",
"AVP  /  Senior Associate, Data Engineer, IBG Digital, Institutional Banking Group (1800044U)",DBS BANK LTD.,"Permanent, Full Time","Manager, Senior Executive",Banking and Finance,5500.0,11000.0,Monthly,"Job Purpose  The Data Engineer will provide big data engineering support to the Institutional Banking Group (IBG) Business Analytics Team in various data science projects. This role’s primary job responsibility is defining the framework and process for preparing data for analytical uses. The right candidate will be one excited by the prospect of designing data engineering solutions from ground up and will support data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. Responsibilities  Create and maintain optimal data pipeline architecture; Assemble large, complex data sets that meet functional / non-functional business requirements; Identify, design, and implement internal process improvements: automating manual processes, Perform ETL/ELT, Data Modelling, Data Profiling, Data Cleansing, Feature Engineering tasks as part of Data Analytics Life Cycle (DALC); Work with stakeholders (data analysts, data scientists, technology support team) to assist with data-related technical issues and support data infrastructure needs; Build processes supporting data transformation, data structures, dependency and workload management ","Job Purpose  The Data Engineer will provide big data engineering support to the Institutional Banking Group (IBG) Business Analytics Team in various data science projects. This role’s primary job responsibility is defining the framework and process for preparing data for analytical uses. The right candidate will be one excited by the prospect of designing data engineering solutions from ground up and will support data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. Responsibilities  Create and maintain optimal data pipeline architecture; Assemble large, complex data sets that meet functional / non-functional business requirements; Identify, design, and implement internal process improvements: automating manual processes, Perform ETL/ELT, Data Modelling, Data Profiling, Data Cleansing, Feature Engineering tasks as part of Data Analytics Life Cycle (DALC); Work with stakeholders (data analysts, data scientists, technology support team) to assist with data-related technical issues and support data infrastructure needs; Build processes supporting data transformation, data structures, dependency and workload management "
Data Engineer,MOKA TECHNOLOGY SOLUTIONS PTE. LTD.,"Permanent, Full Time",Professional,Engineering,5000.0,7500.0,Monthly,"Do you have a passion for data? Are you looking to push the frontiers of innovation and build the Next Big Data Product? We are looking for excellent Data Engineers who are keen to help us manage the end-to-end data pipeline and drive big data solutions.  You will:  Design, implement and manage end to end data pipelines (ETL, data streaming and warehousing) so as to make data easily accessible for analysis. Integrate with third party APIs for accessing external data. Create and maintain data warehouses for reporting or analysis. Consult and partner with engineering and product teams to execute data-related product initiatives. Ability to quickly resolve performance and systems incidents. Evaluate the latest monitoring and automation tools. ","Do you have a passion for data? Are you looking to push the frontiers of innovation and build the Next Big Data Product? We are looking for excellent Data Engineers who are keen to help us manage the end-to-end data pipeline and drive big data solutions.  You will:  Design, implement and manage end to end data pipelines (ETL, data streaming and warehousing) so as to make data easily accessible for analysis. Integrate with third party APIs for accessing external data. Create and maintain data warehouses for reporting or analysis. Consult and partner with engineering and product teams to execute data-related product initiatives. Ability to quickly resolve performance and systems incidents. Evaluate the latest monitoring and automation tools. "
Data Engineer (Python developer),NTT DATA SINGAPORE PTE. LTD.,Contract,"Executive, Junior Executive, Senior Executive",Information Technology,5000.0,7500.0,Monthly," Own, develop and enhance the prototype cost allocation model for Singapore     Taking over the existing model and making further modifications / enhancements as required   Customise the prototype as required for rolling out the new cost allocation design to other countries     Implementing modifications / enhancements to the prototype as required for each country   "," Own, develop and enhance the prototype cost allocation model for Singapore     Taking over the existing model and making further modifications / enhancements as required   Customise the prototype as required for rolling out the new cost allocation design to other countries     Implementing modifications / enhancements to the prototype as required for each country   "
Mobile Network & Data Support Engineer,CHANDLER MACLEOD GROUP PTE. LTD.,Contract,Senior Executive,Information Technology,4000.0,8000.0,Monthly,"A well-established global brand is looking for a high calibre Mobile Network and Data Support Engineer in Singapore. This role is responsible providing technical support to improve customer’s experience with front-line support team. Ideally you should have experience helping users to troubleshoot network connectivity issues, experience with NOC and highly knowledgeable with IP networking.   Responsibilities:  Deliver technical support inline with a set of departmental service levels through a scaled ticketing systems Perform product integrations with mobile operators remotely, involving traffic zero rating Work with the mobile infrastructure engineering team, finding or escalating issues (TCP/IP, VPN, SMPP, SS7) Analysing protocol logs, add new IP address and network traces Troubleshooting networking and service issues on Linux based operating systems Liaise with carrier / operator partners on technical issues around connectivity ","A well-established global brand is looking for a high calibre Mobile Network and Data Support Engineer in Singapore. This role is responsible providing technical support to improve customer’s experience with front-line support team. Ideally you should have experience helping users to troubleshoot network connectivity issues, experience with NOC and highly knowledgeable with IP networking.   Responsibilities:  Deliver technical support inline with a set of departmental service levels through a scaled ticketing systems Perform product integrations with mobile operators remotely, involving traffic zero rating Work with the mobile infrastructure engineering team, finding or escalating issues (TCP/IP, VPN, SMPP, SS7) Analysing protocol logs, add new IP address and network traces Troubleshooting networking and service issues on Linux based operating systems Liaise with carrier / operator partners on technical issues around connectivity "
DATA CENTER ENGINEER,Company Undisclosed,Permanent,Senior Executive,Engineering,,,,"As a Data Center Engineer in the Technical Operations group, the ideal candidate will have the knowledge and experience to work in a fast-paced Data Center environment. They will be responsible for the day-to-day operations and projects to ensure the continued success.   Conduct all day-to-day operations for the Data Center   Install all racks and enclosures for equipment   Prepare all equipment to be installed in racks and other enclosures   Update cable the management system as changes are made to the data center cable plant   Update asset management system as equipment is added and removed from the Datacenter   Define tasks for each project and work with all TechOps teams to meet project timelines   Provide status to team members and management on the completion of all tasks   Provide all information to assist with Data Center capacity planning for space and power   Manage all vendor resources to complete tasks as defined in SOWs   Research new Data Center infrastructure equipment advancements and recommend changes as needed   Conduct all audits as required by company policies   Coordinate with the warehouse the delivery and shipping of all equipment    ","As a Data Center Engineer in the Technical Operations group, the ideal candidate will have the knowledge and experience to work in a fast-paced Data Center environment. They will be responsible for the day-to-day operations and projects to ensure the continued success.   Conduct all day-to-day operations for the Data Center   Install all racks and enclosures for equipment   Prepare all equipment to be installed in racks and other enclosures   Update cable the management system as changes are made to the data center cable plant   Update asset management system as equipment is added and removed from the Datacenter   Define tasks for each project and work with all TechOps teams to meet project timelines   Provide status to team members and management on the completion of all tasks   Provide all information to assist with Data Center capacity planning for space and power   Manage all vendor resources to complete tasks as defined in SOWs   Research new Data Center infrastructure equipment advancements and recommend changes as needed   Conduct all audits as required by company policies   Coordinate with the warehouse the delivery and shipping of all equipment    "
"VP / AVP, Senior Data Engineer, Group Consumer Banking and Big Data Analytics Technology (180003L2)",DBS BANK LTD.,"Permanent, Full Time","Middle Management, Manager",Banking and Finance,7000.0,14000.0,Monthly," Design and implement key components for highly scalable, distributed data collection and analysis system built for handling petabytes of data in the cloud.  Work with architects from other divisions contributing to this analytics system and mentor team members on best practices in backend infrastructure and distributed computing topics.  Analyze source data and data flows, working with structured and unstructured data. Manipulate high-volume, high-dimensionality data from varying sources to highlight patterns, anomalies, relationships and trends Analyze and visualize diverse sources of data, interpret results in the business context and report results clearly and concisely. Apply data mining, NLP, and machine learning (both supervised and unsupervised) to improve relevance and personalization algorithms. Work side-by-side with product managers, software engineers, and designers in designing experiments and minimum viable products. Build and optimize classifiers using machine learning techniques and enhance data collection procedures that is relevant for building analytic systems. Discover data sources, get access to them, import them, clean them up, and make them “model-ready”. You need to be willing and able to do your own ETL. Create and refine features from the underlying data. You’ll enjoy developing just enough subject matter expertise to have an intuition about what features might make your model perform better, and then you’ll lather, rinse and repeat. Run regular A/B tests, gather data, perform statistical analysis, draw conclusions on the impact of your optimizations and communicate results to peers and leaders. "," Design and implement key components for highly scalable, distributed data collection and analysis system built for handling petabytes of data in the cloud.  Work with architects from other divisions contributing to this analytics system and mentor team members on best practices in backend infrastructure and distributed computing topics.  Analyze source data and data flows, working with structured and unstructured data. Manipulate high-volume, high-dimensionality data from varying sources to highlight patterns, anomalies, relationships and trends Analyze and visualize diverse sources of data, interpret results in the business context and report results clearly and concisely. Apply data mining, NLP, and machine learning (both supervised and unsupervised) to improve relevance and personalization algorithms. Work side-by-side with product managers, software engineers, and designers in designing experiments and minimum viable products. Build and optimize classifiers using machine learning techniques and enhance data collection procedures that is relevant for building analytic systems. Discover data sources, get access to them, import them, clean them up, and make them “model-ready”. You need to be willing and able to do your own ETL. Create and refine features from the underlying data. You’ll enjoy developing just enough subject matter expertise to have an intuition about what features might make your model perform better, and then you’ll lather, rinse and repeat. Run regular A/B tests, gather data, perform statistical analysis, draw conclusions on the impact of your optimizations and communicate results to peers and leaders. "
Senior Database Consultant - Big Data Engineer,PALO IT SINGAPORE PTE. LTD.,"Permanent, Full Time",Professional,Information Technology,6000.0,12000.0,Monthly,"Your profile & role on the project YOU:  Thrive on challenge. When was the last time you failed? Are curious & always learning. What are you up to right now? Can deal with constant change. When were you last surprised? Have mastered at least one skill of your trade but you’re not defined by it. What can you teach us? Can you wear many hats?  YOU AGAIN: The DevOps Architect will install, maintain, and support an on-premises cloud infrastructure and apply DevOps practices and solutions. The person will also implement cloud-related and DevOps technologies such as AWS/Puppet/Chef/Elk/Azure/Openstack. Other infrastructure related activities such as maintaining the company internal server infrastructure and respond to consultant requests when required will be expected.  Install, maintain, and support on-premises and off-premises cloud stack. Configure, maintain, and support the cloud-related infrastructures. Act as a system administrator on different OSes (e.g. RHEL, Opensolaris, Ubuntu, etc.) and help teams deploy their application and automate their development and releases on the cloud. Ability to develop solutions and self-learn new tools and technologies. Document, and share knowledge on developed DevOps solutions.  STILL YOU:  Unix / Linux / Bash knowledge Very good understanding of cloud computing (e.g. Technologies, Deployment, costing, HA/DR, etc.) Good understanding of DevOps principles (e.g. testing automation, BDD, TDD, Release automation, CI/CD, etc.) 2 years experience with cloud deployment (e.g. Openstack, VMWare, AWS, Azure, Terraform, etc.) 1 year experience with testing automation (e.g. Maven, Selenium, HP QC, LoadRunner) 1 year experience with release automation process (e.g. CA-RA, Jenkins, etc.) 1 year experience with Configuration Management (e.g. Ansible, SaltStack, Puppet/Chef, etc.) 1 year experience with monitoring tools (e.g. ELK, Prometheus, Grafana and Splunk) Experience with developing and implementing processes to handle releases from Development to Operations while respecting internal rules, and offering solutions for rollback) Experience with designing an architecture to implement development-to-production workflows. Knowledge of SRE, Containers, Kubernetes, Openshift is a plus. Good understanding of microservice architecture and DevOps practices that support. Strong RDMS and NoSQL skill in deploying and fine tuning such as MySQL, Oracle, Elasticsearch.  Your role at PALO IT You will be invited to take part in R&D works done within our Practices. You will have the chance to assist or be a speaker at must-attend international IT conferences. You will have the opportunity to write articles for our Blog or specialized press. Genuine ambassador of PALO IT, you will present our offers and take an active role in the development of the company.  Your technical environment # Cloud and DevOps based technologies (AWS/Puppet/Chef/Elk/Azure/Opencloud) # DevOps practices # Linux OS, Shell Scripting, SQL # Agile and scrum environment","Your profile & role on the project YOU:  Thrive on challenge. When was the last time you failed? Are curious & always learning. What are you up to right now? Can deal with constant change. When were you last surprised? Have mastered at least one skill of your trade but you’re not defined by it. What can you teach us? Can you wear many hats?  YOU AGAIN: The DevOps Architect will install, maintain, and support an on-premises cloud infrastructure and apply DevOps practices and solutions. The person will also implement cloud-related and DevOps technologies such as AWS/Puppet/Chef/Elk/Azure/Openstack. Other infrastructure related activities such as maintaining the company internal server infrastructure and respond to consultant requests when required will be expected.  Install, maintain, and support on-premises and off-premises cloud stack. Configure, maintain, and support the cloud-related infrastructures. Act as a system administrator on different OSes (e.g. RHEL, Opensolaris, Ubuntu, etc.) and help teams deploy their application and automate their development and releases on the cloud. Ability to develop solutions and self-learn new tools and technologies. Document, and share knowledge on developed DevOps solutions.  STILL YOU:  Unix / Linux / Bash knowledge Very good understanding of cloud computing (e.g. Technologies, Deployment, costing, HA/DR, etc.) Good understanding of DevOps principles (e.g. testing automation, BDD, TDD, Release automation, CI/CD, etc.) 2 years experience with cloud deployment (e.g. Openstack, VMWare, AWS, Azure, Terraform, etc.) 1 year experience with testing automation (e.g. Maven, Selenium, HP QC, LoadRunner) 1 year experience with release automation process (e.g. CA-RA, Jenkins, etc.) 1 year experience with Configuration Management (e.g. Ansible, SaltStack, Puppet/Chef, etc.) 1 year experience with monitoring tools (e.g. ELK, Prometheus, Grafana and Splunk) Experience with developing and implementing processes to handle releases from Development to Operations while respecting internal rules, and offering solutions for rollback) Experience with designing an architecture to implement development-to-production workflows. Knowledge of SRE, Containers, Kubernetes, Openshift is a plus. Good understanding of microservice architecture and DevOps practices that support. Strong RDMS and NoSQL skill in deploying and fine tuning such as MySQL, Oracle, Elasticsearch.  Your role at PALO IT You will be invited to take part in R&D works done within our Practices. You will have the chance to assist or be a speaker at must-attend international IT conferences. You will have the opportunity to write articles for our Blog or specialized press. Genuine ambassador of PALO IT, you will present our offers and take an active role in the development of the company.  Your technical environment # Cloud and DevOps based technologies (AWS/Puppet/Chef/Elk/Azure/Opencloud) # DevOps practices # Linux OS, Shell Scripting, SQL # Agile and scrum environment"
"VP / AVP, Machine Learning Engineer, Group Consumer Banking and Big Data Analytics Tech (180003YE)",DBS BANK LTD.,"Permanent, Full Time","Middle Management, Manager",Banking and Finance,7000.0,14000.0,Monthly,"Job Purpose    Build and improve machine learning and analytics platform. Work with data scientists to create, optimize and productionize of machine learning models for various business units within the organization. Keep innovating and optimizing data and machine learning workflow to enable data-driven business activities at large scale.    Responsibilities   Build and improve machine learning and analytics platform.       Apply cutting edge technologies and tool chain in big data and machine learning to build machine learning and analytics platform. Keep innovating and optimizing the machine learning workflow, from data exploration, model experimentation/prototyping to production. Provide engineering solution and framework to support machine learning and data-driven business activities at large scale. Perform R&D on new technologies and solutions to improve accessibility, scalability, efficiency and us abilities of machine learning and analytics platform.     Work with data scientists to build end-to-end machine learning and analytics solution to solve business challenges.       Turn advanced machine learning models created by data scientists into end-to-end production grade system. Build analytics platform components to support data collection, exploratory, and integration from various sources being data API, RDBMS, or big data platform. Optimize efficiency of machine learning algorithm by applying state-of-the-art technologies, i.e. distributed computing, concurrent programming, or GPU parallel computing.      Establish, apply and maintain best practices and principles of machine learning engineering.       Study and evaluate the state of the art technologies, tools, and frameworks of machine learning engineering. Contribute in creation of blueprint and reference architecture for various machine learning use cases. Support the organization in transformation towards a data driven business culture.     Work Relationships  Internal        Work closely with data scientists, business team, and project managers to provide machine learning and data-driven business solution.  Collaborate with other technology teams to build platform and framework to enable machine learning and data analytics activities at large scale     External        Maintain engineering principles and best practices of machine learning framework and technologies.   ","Job Purpose    Build and improve machine learning and analytics platform. Work with data scientists to create, optimize and productionize of machine learning models for various business units within the organization. Keep innovating and optimizing data and machine learning workflow to enable data-driven business activities at large scale.    Responsibilities   Build and improve machine learning and analytics platform.       Apply cutting edge technologies and tool chain in big data and machine learning to build machine learning and analytics platform. Keep innovating and optimizing the machine learning workflow, from data exploration, model experimentation/prototyping to production. Provide engineering solution and framework to support machine learning and data-driven business activities at large scale. Perform R&D on new technologies and solutions to improve accessibility, scalability, efficiency and us abilities of machine learning and analytics platform.     Work with data scientists to build end-to-end machine learning and analytics solution to solve business challenges.       Turn advanced machine learning models created by data scientists into end-to-end production grade system. Build analytics platform components to support data collection, exploratory, and integration from various sources being data API, RDBMS, or big data platform. Optimize efficiency of machine learning algorithm by applying state-of-the-art technologies, i.e. distributed computing, concurrent programming, or GPU parallel computing.      Establish, apply and maintain best practices and principles of machine learning engineering.       Study and evaluate the state of the art technologies, tools, and frameworks of machine learning engineering. Contribute in creation of blueprint and reference architecture for various machine learning use cases. Support the organization in transformation towards a data driven business culture.     Work Relationships  Internal        Work closely with data scientists, business team, and project managers to provide machine learning and data-driven business solution.  Collaborate with other technology teams to build platform and framework to enable machine learning and data analytics activities at large scale     External        Maintain engineering principles and best practices of machine learning framework and technologies.   "
Data Centre Facilities Management Engineer,ENGIE ITS  PTE. LTD.,Permanent,Junior Executive,Engineering,2300.0,4000.0,Monthly,"Responsibilities: Manage data centre operations and facilities Plan and implement predictive and preventive programmes Manage sub-contractors to carry out maintenance works Project manage facility development Provide engineering/technical expertise and value engineering to customers Respond to service call and ensure resolution of the problems Prepare weekly reports, incident reports and O&M procedures Ensure critical system availability to meet SLA.","Responsibilities: Manage data centre operations and facilities Plan and implement predictive and preventive programmes Manage sub-contractors to carry out maintenance works Project manage facility development Provide engineering/technical expertise and value engineering to customers Respond to service call and ensure resolution of the problems Prepare weekly reports, incident reports and O&M procedures Ensure critical system availability to meet SLA."
Big Data Engineer,Company Undisclosed,Contract,Professional,Information Technology,6500.0,9000.0,Monthly,"• Evaluate and renew implemented big data architecture solutions to ensure their relevance and effectiveness in supporting business needs and growth. • Design, develop and maintain data pipelines, with a focus on writing scalable, clean, and fault-tolerant code to handle disparate data sources, process large volume of structured / unstructured data from various sources. • Understand business requirements and solution designs to develop and implement solutions that adhere to big data architectural guidelines and address business requirements • Support and maintain previously implemented big data projects, as well as provide guidance and consultation on other projects in active development as needed • Drive optimization, testing and tooling to improve data quality • Document and communicate technical complexities completely and clearly to team members and other key stakeholders","• Evaluate and renew implemented big data architecture solutions to ensure their relevance and effectiveness in supporting business needs and growth. • Design, develop and maintain data pipelines, with a focus on writing scalable, clean, and fault-tolerant code to handle disparate data sources, process large volume of structured / unstructured data from various sources. • Understand business requirements and solution designs to develop and implement solutions that adhere to big data architectural guidelines and address business requirements • Support and maintain previously implemented big data projects, as well as provide guidance and consultation on other projects in active development as needed • Drive optimization, testing and tooling to improve data quality • Document and communicate technical complexities completely and clearly to team members and other key stakeholders"
Data Engineer,INTELLECT MINDS PTE. LTD.,Full Time,Executive,Information Technology,5000.0,7000.0,Monthly,"Company Overview Intellect Minds is a Singapore-based company since 2008, specializing in talent acquisition, application development, and training. We serve BIG MNCs and well-known clients in talent acquisition, application development, and training needs for Singapore, Malaysia, Brunei, Vietnam and Thailand. Our client is an establish company a, leader within their industry, is now looking for a Data Engineer to join their esteemed organization. Job Descriptions: Responsibilities • Create and maintain optimal data pipeline architecture. • Assemble large, complex data sets that meet functional / non-functional business requirements. • Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Cassandra, Hadoop and other Big Data Technologies. • Build data pipeline on premise and on Google Cloud Platform. • Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics. • Work with stakeholders including the Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. • Work with data and analytics experts to strive for greater functionality in our data systems.","Company Overview Intellect Minds is a Singapore-based company since 2008, specializing in talent acquisition, application development, and training. We serve BIG MNCs and well-known clients in talent acquisition, application development, and training needs for Singapore, Malaysia, Brunei, Vietnam and Thailand. Our client is an establish company a, leader within their industry, is now looking for a Data Engineer to join their esteemed organization. Job Descriptions: Responsibilities • Create and maintain optimal data pipeline architecture. • Assemble large, complex data sets that meet functional / non-functional business requirements. • Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Cassandra, Hadoop and other Big Data Technologies. • Build data pipeline on premise and on Google Cloud Platform. • Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics. • Work with stakeholders including the Product, Data and Design teams to assist with data-related technical issues and support their data infrastructure needs. • Work with data and analytics experts to strive for greater functionality in our data systems."
Data Engineer,AIA SINGAPORE PRIVATE LIMITED,Full Time,Executive,Information Technology,3000.0,6000.0,Monthly," You will be designing, developing and testing ETL Mappings, Mapplets, Workflows and Worklets You will be developing data pipelines to extract, transform and aggregate data that can scale to petabytes, elastically, with low latency and high availability. "," You will be designing, developing and testing ETL Mappings, Mapplets, Workflows and Worklets You will be developing data pipelines to extract, transform and aggregate data that can scale to petabytes, elastically, with low latency and high availability. "
Data Platform Engineer,SILOT PTE. LTD.,Full Time,Professional,Sciences / Laboratory / R&D,4000.0,7000.0,Monthly,"Role   As data engineer at Silot, you design and implement distributed backend services that processes data in real-time, with focus on scalability, data quality and integration of machine learning.   You feel natural at extracting information out of heterogeneous data from many sources. You can plan, build and maintain distributed, service-oriented and event-driven data platform for real-time processing. You like delivering accurate data components that people rely on. You optimize architecture and processes for performance and stability.   To visualize what this position is like, think ""building systems,"" not ""processing data"" (even though your day will involve aspects of both).","Role   As data engineer at Silot, you design and implement distributed backend services that processes data in real-time, with focus on scalability, data quality and integration of machine learning.   You feel natural at extracting information out of heterogeneous data from many sources. You can plan, build and maintain distributed, service-oriented and event-driven data platform for real-time processing. You like delivering accurate data components that people rely on. You optimize architecture and processes for performance and stability.   To visualize what this position is like, think ""building systems,"" not ""processing data"" (even though your day will involve aspects of both)."
Senior Data Engineer,TITANSOFT PTE. LTD.,Permanent,Executive,Professional Services,4000.0,8000.0,Monthly,"If you believe data makes the world go round, we believe we have found the one we are looking for. Our data and research team are the ultimate magicians of data. They throw data in the system, wave their hands around the keyboard, and pull out a never-ending stream of business value. Our team have opportunities to build efficient and reliable data pipelines that move data across systems. Our team are part mathematician, part computer scientist, and part interpreters- magicians of data. If you are interested to work some magic with our data, drop us an owl. What a Senior Data Engineer does in Titansoft  Partner with internal stakeholders to understand business requirements Work with cross-functional data and product teams to build efficient and scalable data solutions Design, build, optimize, launch and support new and existing data models in production Build scalable solutions of real-time data streaming and static analysis Setup network for deploy cluster and troubleshooting Write Linux script programming to assist in auto deploy and system health monitoring Design and build reliable Hadoop system ","If you believe data makes the world go round, we believe we have found the one we are looking for. Our data and research team are the ultimate magicians of data. They throw data in the system, wave their hands around the keyboard, and pull out a never-ending stream of business value. Our team have opportunities to build efficient and reliable data pipelines that move data across systems. Our team are part mathematician, part computer scientist, and part interpreters- magicians of data. If you are interested to work some magic with our data, drop us an owl. What a Senior Data Engineer does in Titansoft  Partner with internal stakeholders to understand business requirements Work with cross-functional data and product teams to build efficient and scalable data solutions Design, build, optimize, launch and support new and existing data models in production Build scalable solutions of real-time data streaming and static analysis Setup network for deploy cluster and troubleshooting Write Linux script programming to assist in auto deploy and system health monitoring Design and build reliable Hadoop system "
Data Engineer,WOODPECKER ASIA TECH PTE. LTD.,Permanent,Professional,Information Technology,4000.0,8000.0,Monthly,"Our Data Engineer (in the analytics team) is the person responsible for enhancing our analytics and performance management framework. You’ll be building our data infrastructure - like databases and large-scale data processing tools -  on the Google Cloud Platform.  You’ll be a perfect fit in this role if you’re an eager learner, have prior experience in quantitative domains, and if you’re keen to be a team player in a dynamic start-up. You’ll get to:  Design, construct, install, test and maintain highly scalable data management systems Employ a variety scripting languages and tools to marry systems together Make sure our systems meet business requirements and industry practices Research opportunities for data acquisition and new uses for existing data Develop data set processes for data modelling, mining and production Integrate new data management technologies and software engineering tools into existing structures Create custom software components and analytics applications Install and update disaster recovery procedures Recommend ways to improve data reliability, efficiency and quality Collaborate with data architects, modelers and IT team members on project goals Build high-performance algorithms, prototypes, predictive models and proof of concepts ","Our Data Engineer (in the analytics team) is the person responsible for enhancing our analytics and performance management framework. You’ll be building our data infrastructure - like databases and large-scale data processing tools -  on the Google Cloud Platform.  You’ll be a perfect fit in this role if you’re an eager learner, have prior experience in quantitative domains, and if you’re keen to be a team player in a dynamic start-up. You’ll get to:  Design, construct, install, test and maintain highly scalable data management systems Employ a variety scripting languages and tools to marry systems together Make sure our systems meet business requirements and industry practices Research opportunities for data acquisition and new uses for existing data Develop data set processes for data modelling, mining and production Integrate new data management technologies and software engineering tools into existing structures Create custom software components and analytics applications Install and update disaster recovery procedures Recommend ways to improve data reliability, efficiency and quality Collaborate with data architects, modelers and IT team members on project goals Build high-performance algorithms, prototypes, predictive models and proof of concepts "
Data Engineer,TITANSOFT PTE. LTD.,Permanent,Executive,Professional Services,3000.0,8000.0,Monthly,"If you believe data makes the world go round, we believe we have found the one we are looking for. Our data and research team are the ultimate managers of data. Others see meaningless figures but they see value. Our team are part of mathematician, part of computer scientist, and part of interpreters. Of data. What a Data Engineer does in Titansoft Manage data warehouse with plans for a business vertical or a group of business verticals Generate and manage all allocated data sets including ensuring its quality based on requirements Work with our Data Infrastructure team to triage and resolve infrastructure issues Manage the delivery of high impact dashboards and data visualisation diagrams","If you believe data makes the world go round, we believe we have found the one we are looking for. Our data and research team are the ultimate managers of data. Others see meaningless figures but they see value. Our team are part of mathematician, part of computer scientist, and part of interpreters. Of data. What a Data Engineer does in Titansoft Manage data warehouse with plans for a business vertical or a group of business verticals Generate and manage all allocated data sets including ensuring its quality based on requirements Work with our Data Infrastructure team to triage and resolve infrastructure issues Manage the delivery of high impact dashboards and data visualisation diagrams"
Data Engineer,BLUE STAR INFOSTACK SOLUTIONS PTE. LTD.,Contract,Executive,Information Technology,3500.0,6000.0,Monthly,"Data Engineer  – Location Singapore    6+ years Experience in ETL / BI Technologies Hands on experience of writing complex SQL queries Experience with Amazon Kinesis, Hadoop, DynamoDB, Hive, and/or Spark a plus Understanding of data warehousing & databases is critical Ability to incorporate a variety of data sources in an analysis (HDFS, file, database, JSON, HTML, etc) Experience with data visualization tools (Tableau) Experience in requirement analysis, system design, development and testing Able to perform independent code reviews and execute unit tests on modules developed by self & other junior team members on the project. Expertise in coding, system testing, implementation and maintenance, performance tuning, go-live support and post-production support. Ensure the data is secure by creating and updating profiles, rules and Business reports timely. Strong communication and collaboration skills to understand customer needs and deliver solutions in alignment with business needs   ","Data Engineer  – Location Singapore    6+ years Experience in ETL / BI Technologies Hands on experience of writing complex SQL queries Experience with Amazon Kinesis, Hadoop, DynamoDB, Hive, and/or Spark a plus Understanding of data warehousing & databases is critical Ability to incorporate a variety of data sources in an analysis (HDFS, file, database, JSON, HTML, etc) Experience with data visualization tools (Tableau) Experience in requirement analysis, system design, development and testing Able to perform independent code reviews and execute unit tests on modules developed by self & other junior team members on the project. Expertise in coding, system testing, implementation and maintenance, performance tuning, go-live support and post-production support. Ensure the data is secure by creating and updating profiles, rules and Business reports timely. Strong communication and collaboration skills to understand customer needs and deliver solutions in alignment with business needs   "
"Manager (Data Engineer), Decision Science, Credit Cards & Personal Financing",CIMB BANK BERHAD,Permanent,Manager,Information Technology,5000.0,6000.0,Monthly," Design, implement, and optimize the data pipelines from various channels using the latest technologies. Deploy dimensional data model to support functional and analytical business requirements. Partner with data analysts, business subject-matter experts and cross-functional technology teams to deliver end-to-end analytics solutions. Determine reporting needs to be integrated with business intelligence visualization tools and manage reports distribution via content management tools. Research, evaluate, and recommend technical solutions for data collection, processing, and reporting. Data cleansing, scraping unstructured data and converting them into structured/usable data. "," Design, implement, and optimize the data pipelines from various channels using the latest technologies. Deploy dimensional data model to support functional and analytical business requirements. Partner with data analysts, business subject-matter experts and cross-functional technology teams to deliver end-to-end analytics solutions. Determine reporting needs to be integrated with business intelligence visualization tools and manage reports distribution via content management tools. Research, evaluate, and recommend technical solutions for data collection, processing, and reporting. Data cleansing, scraping unstructured data and converting them into structured/usable data. "
Data Engineer,INFOGAIN PTE. LTD.,Contract,Executive,Information Technology,3500.0,6000.0,Monthly,"Data Engineer  – Location Singapore  6+ years Experience in ETL / BI Technologies Hands on experience of writing complex SQL queries Experience with Amazon Kinesis, Hadoop, DynamoDB, Hive, and/or Spark a plus Understanding of data warehousing & databases is critical Ability to incorporate a variety of data sources in an analysis (HDFS, file, database, JSON, HTML, etc) Experience with data visualization tools (Tableau) Experience in requirement analysis, system design, development and testing Able to perform independent code reviews and execute unit tests on modules developed by self & other junior team members on the project. Expertise in coding, system testing, implementation and maintenance, performance tuning, go-live support and post-production support. Ensure the data is secure by creating and updating profiles, rules and Business reports timely. Strong communication and collaboration skills to understand customer needs and deliver solutions in alignment with business needs   ","Data Engineer  – Location Singapore  6+ years Experience in ETL / BI Technologies Hands on experience of writing complex SQL queries Experience with Amazon Kinesis, Hadoop, DynamoDB, Hive, and/or Spark a plus Understanding of data warehousing & databases is critical Ability to incorporate a variety of data sources in an analysis (HDFS, file, database, JSON, HTML, etc) Experience with data visualization tools (Tableau) Experience in requirement analysis, system design, development and testing Able to perform independent code reviews and execute unit tests on modules developed by self & other junior team members on the project. Expertise in coding, system testing, implementation and maintenance, performance tuning, go-live support and post-production support. Ensure the data is secure by creating and updating profiles, rules and Business reports timely. Strong communication and collaboration skills to understand customer needs and deliver solutions in alignment with business needs   "
Data Engineer,DATASPARK PTE. LTD.,Permanent,"Executive, Senior Executive",Information Technology,3500.0,10000.0,Monthly,"Responsibilities  design and implement scalable and robust software platform for ingesting and transforming telco network datasets in (near) real-time using a variety of open-source and proprietary Big Data technologies recommend and implement ways to improve data reliability, efficiency and quality collaborate with product management, sales and marketing, and solution delivery teams to support the objectives that customer requirements are well managed and reflected in product releases support the deployment of DataSpark software within clients' IT environment working closely with stakeholders to ensure high standards of data governance during implementation serve as technical subject matter expert in latest big data technologies ","Responsibilities  design and implement scalable and robust software platform for ingesting and transforming telco network datasets in (near) real-time using a variety of open-source and proprietary Big Data technologies recommend and implement ways to improve data reliability, efficiency and quality collaborate with product management, sales and marketing, and solution delivery teams to support the objectives that customer requirements are well managed and reflected in product releases support the deployment of DataSpark software within clients' IT environment working closely with stakeholders to ensure high standards of data governance during implementation serve as technical subject matter expert in latest big data technologies "
